{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Assignment-03 First Step of Machine Learning: Model and Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "同学们，今天我们的学习了基本的机器学习概念，相比你已经对机器学习的这些方法有一个基本的认识了。值得说明的是，机器学习不仅仅是一系列方法，更重要的是一种思维体系，即：依据以往的、现有的数据，构建某种方法来解决未见过的问题。而且决策树，贝叶斯只是实现这个目标的一个方法，包括之后的神经网络。很有可能有一天，神经网络也会被淘汰，但是重要的是我们要理解机器学习的目标，就是尽可能的自动化解决未知的问题。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](https://timgsa.baidu.com/timg?image&quality=80&size=b9999_10000&sec=1571556399207&di=4a97dc15ad08dd49d3748d1edf6109b3&imgtype=0&src=http%3A%2F%2Fc.hiphotos.baidu.com%2Fzhidao%2Fwh%3D450%2C600%2Fsign%3Dae742c6aedcd7b89e93932873a146e91%2F5d6034a85edf8db1b16050c40223dd54574e74c7.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part-1 Programming Review 编程回顾"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1. Re-code the Linear-Regression Model using scikit-learning(10 points)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<评阅点>： \n",
    "> + 是否完成线性回归模型 (4')\n",
    "+ 能够进行预测新数据(3')\n",
    "+ 能够进行可视化操作(3')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.26382025, 0.35099654],\n",
       "       [0.3738004 , 0.59897399],\n",
       "       [0.23911814, 0.13725465],\n",
       "       [0.15779089, 0.68876687],\n",
       "       [0.95316475, 0.45283788],\n",
       "       [0.24513816, 0.45379304],\n",
       "       [0.82326399, 0.59538311],\n",
       "       [0.65012542, 0.40951936],\n",
       "       [0.85683783, 0.07386097],\n",
       "       [0.56698528, 0.74422778],\n",
       "       [0.81378933, 0.08070798],\n",
       "       [0.2643952 , 0.76738531],\n",
       "       [0.01273642, 0.9057228 ],\n",
       "       [0.67744126, 0.97117006],\n",
       "       [0.32697038, 0.26090342],\n",
       "       [0.63368412, 0.14742352],\n",
       "       [0.46707307, 0.02064729],\n",
       "       [0.02342281, 0.96033333],\n",
       "       [0.06852314, 0.22864411],\n",
       "       [0.40909485, 0.63064174]])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "random_data = np.random.random((20, 2))\n",
    "random_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = random_data[:, 0]\n",
    "y = random_data[:, 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def assuming_function(x):#生成一个随机的线性函数\n",
    "    return 13.4 * x + 5 + random.randint(-5, 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = [assuming_function(x) for x in X] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.collections.PathCollection at 0x24295d3c630>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.scatter(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = np.array(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6915103022742294"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "reg = LinearRegression().fit(X.reshape(-1, 1), y) #reshape(-1,1)将数据变为一列\n",
    "reg.score(X.reshape(-1, 1), y) #训练模型"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "可以看出这个拟合的是相当差，毕竟原本图中的数据就不是线性的而是散乱的趋势。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([15.33334875])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reg.coef_ #返回参数 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3.696699545981595"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reg.intercept_ #返回截距"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "#定义一个函数返回一个线性回归模型，这里的f(x)表示是预测值\n",
    "def f(x):\n",
    "    return reg.coef_ * x + reg.intercept_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.collections.PathCollection at 0x24295fd24e0>"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.scatter(X, y) #画出实际值"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2. Complete the unfinished KNN Model using pure python to solve the previous Line-Regression problem. (8 points)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<评阅点>:\n",
    "> + 是否完成了KNN模型 (4')\n",
    "+ 是否能够预测新的数据 (4')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model(X, y):\n",
    "    return [(Xi, yi) for Xi, yi in zip(X, y)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.spatial.distance import cosine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def distance(x1, x2):\n",
    "    return cosine(x1, x2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(x, k = 5):\n",
    "    most_similars = sorted(model(X, y), key = lambda xi:distance(xi[0], x))[:k]\n",
    "    \n",
    "    y_hats = [_y for x, _y in most_similars]\n",
    "    \n",
    "    print(most_similars)\n",
    "    \n",
    "    return np.mean(y_hats)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([0.26382025, 0.3738004 , 0.23911814, 0.15779089, 0.95316475,\n",
       "        0.24513816, 0.82326399, 0.65012542, 0.85683783, 0.56698528,\n",
       "        0.81378933, 0.2643952 , 0.01273642, 0.67744126, 0.32697038,\n",
       "        0.63368412, 0.46707307, 0.02342281, 0.06852314, 0.40909485]),\n",
       " array([ 8.53519134,  6.0089254 , 11.20418301,  8.11439787, 17.77240762,\n",
       "         9.28485136, 14.03173747,  9.71168059, 19.48162695,  7.59760276,\n",
       "        19.90477704,  4.54289568,  5.170668  , 19.07771284, 10.38140316,\n",
       "        10.49136718,  9.25877909,  2.31386566,  2.91821003, 13.48187097]))"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "myself_knn = model(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(0.5669852805117798, 7.59760275885785), (0.3738004028520784, 6.008925398217851), (0.23911813521320924, 11.204183011857005), (0.15779088550294018, 8.114397865739399), (0.823263990302743, 14.031737470056754)]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "9.391369300945772"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predict(0.9)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3. Re-code the Decision Tree, which could sort the features by salience. (12 points)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "信息熵"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$ Entropy = -\\sum_i^n Pr(x_i) log(Pr(x_i)) $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "基尼系数"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$ Gini = 1 - \\sum_{i=1}^J P_i^2 $$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[2, 2, 2, 2, 2, 2, 2, 3]"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#做一个信息熵的测试，假设有如下的数据集\n",
    "[1, 1, 2, 3, 3, 4, 5, 5] \n",
    "[1, 1, 1, 1, 3, 3, 3, 2]\n",
    "[2, 2, 2, 2, 2, 2, 2, 2]\n",
    "[2, 2, 2, 2, 2, 2, 2, 3]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "from icecream import ic  # 是用作调试的包，能打印出变量名"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def entropy(elements):\n",
    "    '''群体混乱程度'''\n",
    "    counter = Counter(elements) #统计elements出现的次数\n",
    "    probs = [counter[c]/len(elements) for c in set(elements)] #c的次数/总次数\n",
    "    ic(probs) #调试概率\n",
    "    return -sum(p * np.log(p) for p in probs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ic| probs: [0.25, 0.125, 0.25, 0.125, 0.25]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "1.559581156259877"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "entropy([1, 1, 2, 3, 3, 4, 5, 5] )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ic| probs: [0.5, 0.125, 0.375]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.9743147528693494"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "entropy([1, 1, 1, 1, 3, 3, 3, 2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ic| probs: [1.0]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "-0.0"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "entropy([2, 2, 2, 2, 2, 2, 2, 2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ic| probs: [0.875, 0.125]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.37677016125643675"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "entropy([2, 2, 2, 2, 2, 2, 2, 3])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "从上面的测试可以看出数据越混乱，信息熵越高，数据越纯，信息熵越小。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "决策树的分割"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "#用字典的形式创造数据集，包含4个特征\n",
    "mock_data = {\n",
    "    'gender':['F', 'F', 'F', 'F', 'M', 'M', 'M'],\n",
    "    'income': ['+10', '-10', '+10', '+10', '+10', '+10', '-10'],\n",
    "    'family_number': [1, 1, 2, 1, 1, 1, 2],\n",
    "   # 'pet': [1, 1, 1, 0, 0, 0, 1],\n",
    "    'bought': [1, 1, 1, 0, 0, 0, 1],\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>gender</th>\n",
       "      <th>income</th>\n",
       "      <th>family_number</th>\n",
       "      <th>bought</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>F</td>\n",
       "      <td>+10</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>F</td>\n",
       "      <td>-10</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>F</td>\n",
       "      <td>+10</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>F</td>\n",
       "      <td>+10</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>M</td>\n",
       "      <td>+10</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>M</td>\n",
       "      <td>+10</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>M</td>\n",
       "      <td>-10</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  gender income  family_number  bought\n",
       "0      F    +10              1       1\n",
       "1      F    -10              1       1\n",
       "2      F    +10              2       1\n",
       "3      F    +10              1       0\n",
       "4      M    +10              1       0\n",
       "5      M    +10              1       0\n",
       "6      M    -10              2       1"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset = pd.DataFrame.from_dict(mock_data) \n",
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'现在有一个新的case\\n[F, -10, 2, 1]女性， 低于10k， 有两人，宠物一只，买不买？\\n\\n[F, +10, 2, 0]女性， 高于10k， 有两人，没有宠物，买不买？'"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''现在有一个新的case\n",
    "[F, -10, 2, 1]女性， 低于10k， 有两人，宠物一只，买不买？\n",
    "\n",
    "[F, +10, 2, 0]女性， 高于10k， 有两人，没有宠物，买不买？'''\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<评阅点>\n",
    "> + 是否实现了信息熵 (1' )\n",
    "+ 是否实现了最优先特征点的选择(5')\n",
    "+ 是否实现了持续的特征选则(6')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4. Finish the K-Means using 2-D matplotlib (8 points)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import KMeans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "X1 = [random.randint(0, 100) for _ in range(100)] #0-100随机数\n",
    "X2 = [random.randint(0, 100) for _ in range(100)] #0-100随机数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Text(0.5, 1.0, 'trending')"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.scatter(X1, X2)\n",
    "plt.xlabel('Xlabel')\n",
    "plt.ylabel('Ylabel')\n",
    "plt.title('trending')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_data = [[x1, x2] for x1, x2 in zip(X1, X2)] #训练集"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "cluster = KMeans(n_clusters=6, max_iter=500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "KMeans(algorithm='auto', copy_x=True, init='k-means++', max_iter=500,\n",
       "       n_clusters=6, n_init=10, n_jobs=None, precompute_distances='auto',\n",
       "       random_state=None, tol=0.0001, verbose=0)"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cluster.fit(training_data) #训练模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[10.2       , 45.9       ],\n",
       "       [76.1875    , 43.8125    ],\n",
       "       [30.2       , 16.93333333],\n",
       "       [25.88235294, 74.76470588],\n",
       "       [79.53333333,  9.73333333],\n",
       "       [81.08333333, 82.83333333]])"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cluster.cluster_centers_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([5, 2, 0, 1, 1, 4, 2, 0, 2, 5, 5, 5, 2, 1, 0, 2, 2, 1, 5, 2, 2, 4,\n",
       "       2, 3, 2, 1, 1, 3, 2, 3, 2, 4, 3, 3, 2, 2, 2, 2, 3, 0, 2, 4, 4, 4,\n",
       "       3, 5, 1, 1, 4, 1, 2, 0, 5, 2, 0, 2, 4, 2, 0, 3, 3, 3, 3, 4, 3, 4,\n",
       "       1, 5, 2, 4, 0, 3, 4, 3, 2, 5, 1, 1, 4, 2, 1, 2, 2, 5, 4, 2, 2, 0,\n",
       "       0, 2, 4, 2, 5, 3, 1, 3, 1, 5, 3, 1])"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cluster.labels_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<评阅点>\n",
    "> + 是否完成了KMeans模型，基于scikit-learning (3')\n",
    "+ 是否完成了可视化任务（5'）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "centers = defaultdict(list)\n",
    "for label, location in zip(cluster.labels_, training_data):\n",
    "    centers[label].append(location)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXcAAAD4CAYAAAAXUaZHAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4xLjAsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+17YcXAAAgAElEQVR4nO3df3TddZ3n8ef7JumPtLaFWhQpScpOZWSpIuZU1MEZjZzFQoHdRQ9s1Oqym3N03KGy5yiceg7DatZxzoh1RuGcrKjViaKLLBRkcDDDquseYAo4BmSwCEnoAtIpkiopND8++8f3G0jSe5N77/f35/t6nNNze7+5yf3ce9t3Pp/39/15f805h4iI+KWS9QBERCR+Cu4iIh5ScBcR8ZCCu4iIhxTcRUQ81Jr1AABe/epXu66urqyHISJSKPfff/+/OOc2VPtaLoJ7V1cX+/bty3oYIiKFYmajtb6mtIyIiIcU3EVEPKTgLiLiIQV3EREPKbiLiHhIwV1ExENLBncz+5qZPWtmD805dryZ3WVm+8Pb48LjZmZ/bWaPmdkvzOzMJAcvIgU3OAhdXVCpBLeDg1mPyBv1zNy/AZy74NiVwJBzbjMwFN4HeC+wOfzTB1wfzzBFxDuDg9DXB6Oj4Fxw29enAB+TJYO7c+4nwHMLDl8I7An/vge4aM7xb7rAPcA6MzsxrsGKiEd27YKJifnHJiaC4xJZszn31zjnngYIb08Ij58EPDnncQfCY8cwsz4z22dm+w4ePNjkMESksMbGGjsuDYn7hKpVOVb1Uk/OuQHnXLdzrnvDhqqtEUTEZx0djR2XhjQb3H8zm24Jb58Njx8ATp7zuI3AU80PT0S81d8P7e3zj7W3B8clsmaD+15gR/j3HcCtc45/KKyaOQsYn03fiIjM09sLAwPQ2Qlmwe3AQHDcJxlVBNlSF8g2s+8AfwK8GvgNcDVwC/A9oAMYA97nnHvOzAz4MkF1zQTwEefcku0eu7u7nbpCioh3ZiuC5p44bm+P7ZeYmd3vnOuu+rWlgnsaFNxFxEtdXUGJ50KdnTAyEvnHLxbctUNVRCQpGVYEKbiLiCQlw4ogBXcRkaRkWBGk4C6SFvVRKZ8MK4JycQ1VEe8trJqY7aMC/pX+yXy9vZl8xpq5i6RBfVQkZQruImlQHxVJmYK7SBrUR6V4Cn6ORMFdJA3qo1IsHvSaV3AXSUNZ+qj4woNzJGo/ICKyUKUSzNgXMoOZmfTHU4PaD4iINMKDcyQK7iJJKvhJudLy4ByJgrtIUjw4KVdaHpwjUc5dJCkJt3sVUc5dJAvauCQZUnD3yRODcEsXfLsS3D6h5X+mPDgpJ8Wl4O6LJwbhvj6YGAVccHtfnwJ8ljw4KSfF5XdwL9NM9p92wfSCTRfTE8FxyYYHJ+WkuPxt+Ts7k50NeLMzWYBNHv7nmqhy4m6x45KOjNq9ivg7cy/bTNZaGjsuIl7zN7hP1KhIqHW86Nx0Y8dFxGv+Bvf2GhUJtY4XXXtnY8frVabzFtKcGHbhPv/M0/zohuv4mw+/jy9csp2/+fD7+NEN1/H8M0/HPtyy8De4v6kfWhZUKrS0B8d9lMTrVQWOLCWGXbhPPLiPPZ/8OMNDP+TokSPgHEePHGF46Ifs+eTHeeJBbXBshr/BfVMvbB0IZ64W3G4d8PNkKiTzeus9b6HZfXlFbI37/DNPs/eLn2PqpZeYmZ6fQpyZnmbqpZfY+8XPaQbfBH+rZSAIbL4G82rifr31nLcoW1WSzBdxF+6+H/wvZqamFn3MzNQU+35wC++57KONjq7U/J25S3T1nLcoW1WSzBdxF+4jP737mBn7QjPT0zzy07sbHVnpKbhLbfXk8ctWlSTzRdyFe/TFF+t83JFGR1Z6Cu5SWz15/LJVJcl8EXfhLluxos7HrYwyylLyO+cu0S2Vx3/dNnjs+urHpRwi7MJ9w9nvYnjoh4umZiotLbzh7Hc1O7rSijRzN7NPmNnDZvaQmX3HzFaY2SYzu9fM9pvZd81sWVyDlRx66o7GjovM0X3ev6XSuvgcs9LaSvd5F6U0In80HdzN7CTgz4Bu59zpQAtwCfB54IvOuc3Ab4HL4hio5JRy7hLButeeyAWfuIrW5cuptMxvlVFpaaF1+XIu+MRVrHvtiRmNsLii5txbgZVm1gq0A08D7wZuCr++B9CvXJ8llXNX7XxpbHpzNzv+8sts6TmXZSvbwYxlK9vZ0nMuO/7yy2x6c9ULDckSIl1mz8wuB/qBI8DfA5cD9zjn/iD8+snA34Uz+4Xf2wf0AXR0dLxltNrlyCT/Fta5Q1BRE2UDVRI/U8RDiVxmz8yOAy4ENgGvA1YB763y0Kq/PZxzA865budc94YNG5odhmQty52xInGLoU9OXkSplnkP8IRz7iCAmd0MvB1YZ2atzrkpYCPwVPRhSq5lsTNWJG6zfXJm2ynM9smBQvbkj5JzHwPOMrN2MzOgB/glcDdwcfiYHcCt0YYYgfK2xaTa+eR4NDONXcQ+OXnTdHB3zt1LcOL0AWA4/FkDwKeAK8zsMWA9cEMM42ycOhoWV9k6eqYlhg6OXovYJydvIp1QjUt3d7fbty/mtp63dFW/xFx7J1w0Eu9zFczRsTEOff3rHN57GzMTE1Ta21lzwXbWf+QjLKuzJ0jinhgMcuwTY8GM/U39OpkaVVdXENAX6uyEkZG0R5M/BXx/Fjuh6m9w/3aF6udyDf7DTLzPVSC//8lPOHD5TtzkJMztxtfairW1sfFLu1n9zndmN0BJTqUSzNgXMoOZ8v6feNnCnDsEfXJyfFHzRKplck9522McHRsLAvuRI/MDO8DUFO7IEQ5cvpOjBV2GyhIidnD0XsQ+OXnjb3BX3vYV4YnlQ1dtxb00sehD3eQkh76xJ6WBSaoidnAshd7eIAUzMxPcFjSwg8/BvWxXYqplzonlw8NrYMYWf/zUFIf37k1nbJIuz2amsji/u0KW7UpM1czZEDRztL7f5TMvvJDkiCRLETo4SrH4O3OXwJyNP5Vl9Z00q6xaldRoRCQlCu6+m3MCec2Ww1BZojqqtZU1F1yQ8KBEmqRNWHVTcPfdnBPL69/2HNayeHC3tjbWf3hHGiMTaYw2YTVEwb2IGmmrMOfE8rLjp9h4ye+wNnfsDL61FVu5ko1f2p2fjUwicy3WHiDNGX1BVg/+bmLyVZR2uOH3Hj04yaF7jufw8BpmjlaorFzBmov+Pes/vEOBXfKr1iYsCEo609h8lLONTuXcoeqrKG0VsmzJoHYCElWt9gAtLVDtGqxJtA3IWYuCcu5Q9VWUdrhZtdJVEzeJQ61NWLUurp3ETusCNRdTcC+aKG0VsmrJoItvSBxqbcLq7Kz++I6O+PPjBWrhoOBeNFHaKmTVkkEX35C4VGsPUGtGv21b/NU1BWrhoOBeNFHaKmTVkkFN3CRJtWb0d9wR/8U3CtTCQSdUJXm64LVkoQQtjnVCteziutxgsz9HTdwkCwXKjyfB78ZhcuysebZSBRoLrlF/jpq4Sdr6+6vXpOcwP54Ezdx9F1elStErXnSx9PIpUH48CZq5+y6uSpUiV7zEtXqR4ilxi2PN3H0XV6VKkSteir7qEGmCgrvv4qptL/JlC4u86hBpkoJ73sSdG46rUqXIFS9FXnWINEl17nmievBk6H0VT6nOvSiUG05GkVcdIk1StUyeKDecHNXZS8lo5p4nyg2LSEwU3POkyBUpIpIrCu55otywiMQkUs7dzNYBXwVOBxzwH4FHge8CXcAI8H7n3G8jjbJMlBsWkRhEnbl/CbjTOfeHwJuAR4ArgSHn3GZgKLwvIiIpajq4m9ka4J3ADQDOuaPOueeBC4E94cP2ABdFHaSIiDQmysz9FOAg8HUze9DMvmpmq4DXOOeeBghvT4hhnCIi0oAowb0VOBO43jn3ZuAFGkjBmFmfme0zs30HDx6MMAzxklr0ikQSJbgfAA445+4N799EEOx/Y2YnAoS3z1b7ZufcgHOu2znXvWHDhgjDEO/MtguYGAXcKy16FeBF6tZ0cHfOPQM8aWanhod6gF8Ce4Ed4bEdwK2RRijlU7I2DIODg3R1dVGpVOjq6mJwUL/EJLqo7Qf+CzBoZsuAx4GPEPzC+J6ZXQaMAe+L+BxSNiVqwzA4OEhfXx8T4aXgRkdH6esLLiTSW9KLTEg8StkVcnh4mKGhIcbHx1m7di09PT1s2bIlteeXJdzSFaZkFmjvhItG0h5Norq6uhgdPfa1dnZ2MjIykv6ApFDUFXKO4eFhbrvtNsbHxwEYHx/ntttuY3h4OOORyctK1IZhbKz6aqTWcZF6la4r5NDQEJOTk/OOTU5OMjQ0lPjsXSuGOs3u0P2nXUEqpr0jCOwe7tzt6OioOnPv6FCzOImmdMF9dsZe7/G4zK4YZn+xzK4YAAX4akrShqG/v39ezh2gvb2d/n7/VimSrtKlZdauXdvQ8bgstmIQvzRS/dLb28vAwACdnZ2YGZ2dnQwMDOhkqkRWupl7T0/PvBk0QFtbGz09PYk+b1YrBklXM9Uvvb29CuYSu9LN3Lds2cL27dtfnqmvXbuW7du3J54ayWrFIOnatWvXvBQLwMTEBLt2+VmjL/lVupk7BAE+7Tx3VisGSZeqXyQvSjdzz0pWKwZJV60ql7xUv2g3bHmUcuaelSxWDJKuPFe/aDdsuZRyh6qvVEefD4ODg+zatYuxsTE6Ojro7+/PRfDUblj/LLZDVcHdEwvr6CHI6Sv1I7MqlQrV/r+bGTMzMxmMSKJS+4FFDA8Ps3v3bq655hp2795d2DYEqqOXpeT9fEAtOk/QnFIHd5/6zKiOXpbS399Pe/v8nj15OR9Qy+x5gtHRUZxzL58nUIBfWqmDu0+zXdXRy1KKuBtW+waaV+rg7tNst6enh7a2tnnHVEcvC/X29jIyMsLMzAwjIyO5DuygfQNRlDq4LzbbLVouXnX04qOkzhOUIY9f6jr3WrtGN2/eXMgOjqqjF99s27aN66+/vurxZpWl3r/UM/das939+/fXlYsfPfQCn75lmNOv/iGbrvwBp1/9Qz59yzB//38fKNSsXySv7rjjjoaO1yONPH6tlUGaKwbVuVdxzTXX1Pza1VdfDcDdjz7Lx/72ASanZ5iaeeU9bDEwN827lv2ajS2HAdWbizQridr8pOv9F64MIKhK2rFjB3v27DnmeJST2qpzb9BSlSejh17gY3/7AEcmp+cFdoBpB1O0cPfRf8XhmeVAcStwRLKWRM496Xr/WiuDgYGBVCt/FNyrWKry5H/89HEmpxf/DT+N8fDUCS/fL2IFjkjWkqjNT7rev1Ylz/T0dEOPj0rBvYqlKk9uefCpY2bsCzkq/Hp6/cv3VW8upfDc43D7FfDfN8Kfrwtub78iON6EJGrzk673r7UCaGlpaejxUSnn3oRNV/6A+t41x0dW3q+cu5TD/rvgex+C6UmYmVOQUGmDljZ4/zdh8znZjS8lyrkX2Krl9VWQtjGtenPJrVgrN557PAjskxPzAzsE9ycngq83OYMvklorg+uuuy7VHcKauTfh07cMc+N9Ty6ammmtGJdu7eAzF52e4shE6lNrdtl0sLn9Cnjgm8cG9rkqbfCWHXDeF5oYsVSjmXvM/vPZp9DWsvhb19ZS4T+dvSmlEYk0JvZa7198b/HADsHXf/Hd5n6+NEzBvQmd61dx3QfOZGVbC60Vm/e11oqxsq2F6z5wJp3rV2U0QqlXGbahVxN7z5ajv4/3cRKZgnuT3nXqCdy582wu3drB6uWtmMHq5a1curWDO3eezbtOPWHpHyKZKnM72dhrvZetjvdxEpkXOXddXm5xen+qK/Nl55Rz94PXOXefLriRBL0/tZW5nWzstd5v/3hQ7riYljZ425829/OlYZGDu5m1mNmDZnZ7eH+Tmd1rZvvN7Ltmtiz6MGvz6YIbSdD7U1tRLzsXl1h7ux9/SlDH3tYezNDnqrQFx9//zeBxkoo4Zu6XA4/Muf954IvOuc3Ab4HLYniOmny64EYS9P7UVsTLzuXa5nPgoz8LUi/LXwVmwe1bdgTHS7CBKU8iBXcz2wicB3w1vG/Au4GbwofsAS6K8hxL0eXlFqf3p7YiXnYu944/JcipX3UArn4+uD3vC5qxZyDqzH038ElgtovWeuB559xUeP8AcFK1bzSzPjPbZ2b7Dh482PQAdHm5xen9WVzRLjsnUq+mg7uZnQ8865y7f+7hKg+tWo7jnBtwznU757o3bNjQ7DB0ebkl6P3xT1lr86UxUS6z9w7gAjPbBqwA1hDM5NeZWWs4e98IPBV9mIvT5eUWp/fHH2W5RJxE1/TM3Tl3lXNuo3OuC7gE+AfnXC9wN3Bx+LAdwK2RR5kjRbtwtvgljUvE1aIVQ/3y8F4lcYHsTwE3mtlngQeBGxJ4jkzM1owX7cLZ4o+savO1YqhfXt6rWDYxOef+t3Pu/PDvjzvntjrn/sA59z7n3EtxPEceqGZcspZVbX6WK4aiqfVefeADH0h1Fl/4HappUs24ZC2r2vwy7+Zt1GLvSZr9ixTcG6Ca8fIYHB6ka3cXlWsqdO3uYnA4H/nlrGrzy76btxFLvSdprXgU3BugmvFyGBwepO+2PkbHR3E4RsdH6butL1cBPu3afO3mrV+192qhNFY8Cu4NUM14Oewa2sXE5IKc6eQEu4bKm1/Wbt76zX2vakljxeNFy1+ROFWuqeCq7L0zjJmrZ6p8h0h1sbdWXsDrlr9RqW7dL3HkyjvW1sgv1zheJHmovy6TLFc8pZ65L6xbhyCHrlRLMc3myuemVNrb2hnYPkDvlvr/M8X1c/Im6VmkpE8z9xpUt15sC2fpl//d5bHkynu39DKwfYDOtZ0YRufazsIHdlCtetkksUO1MFS3XlwLZ9ej48deLm/W2HjjlQm9W3oLH8wXUq16uZR65q669WzEkRevVtFSiw+58noslU9XrXq5lDq4q249fXHVkNc7G29va6e/x/9a7Nl8+ujoKM65qjshVateLqUO7qpbT19cNeS1ZuPrV673Lldej3ry6apVL5dSV8tI+uKqIfe1oqVZlUqFav+XzYyZGdXm+0rVMpIbcdWQ+1rR0izl02UhBXdJVX9PP+1tC/K+TebFe7f0MrJzhJmrZxjZOVLawA7Kp8uxFNxLJuluh0v9fM24k6F8uiyknHuJJJ2nVh5cJF3KuXuskZl40t0O1U1RJD9KvUO16Krt0uy7LbxWY5WZcq3a8GZ2cDbyc+L6+SJSP83cC6zRmXLS3Q6L2E0xr1dcEolKwb3AGp0pR61UWSoQxlkJk4a8X3FJJAoF9wJrdKYcpVKlnkBYtEoYnSMQn6lapsDSrE7p2t1VtfNi59pORnaOxPpcadEVl6ToVC3jqTRnyj6eLC3iOQKReqlapoAGhwfZNbSLsfExOtZ20N/Tn3jqo2NtR9WZe5EDYX9Pf9WVT17PEYg0QjP3gsnqJGDRTpbWo2jnCEQaoZx7DjQyE88y953FikFEalss5660TMbythFpMT5eek7EV0rLZCxvG5HKRpuYxFcK7hlLeyOSvEKbmMRnTQd3MzvZzO42s0fM7GEzuzw8fryZ3WVm+8Pb4+Ibrn/i3IikWWhjtIlJfNb0CVUzOxE40Tn3gJm9CrgfuAj4MPCcc+4vzOxK4Djn3KcW+1llPqEa10YktdttnDYxSdElsonJOfe0c+6B8O+/Ax4BTgIuBPaED9tDEPClhrjK8bKchaa5YojzuXT+QgKDQBdBOOwK7xdfLKWQZtYF/AQ4HRhzzq2b87XfOueOSc2YWR/QB9DR0fGW0dFjy/ukflnNQtNcMcT9XFrtSBDI+4C5E6N2YADI/7+BRNsPmNlq4PvATufc4Xq/zzk34Jzrds51b9iwIeowqipTDjrpWeiTh5/ks/d8lrO+fRZv3PNGzvr2WXz2ns/y6bs/ndqKIe7ViTYxCexifmAnvF/88y6R6tzNrI0gsA86524OD//GzE50zj0d5uWfjTrIZjRaP150SW6l/+mBn3LFj69ganqKKTcFwAuTL/D9X32f9le1s/rF1fz+yO/nfU8SdfdJ1Pirdr/sav3bKW7PpFlRqmUMuAF4xDl37Zwv7QV2hH/fAdza/PCaV7ZKiIWz0PUr17OydSUfvPmDkVYtTx5+kit+fAUvTr34cmCfNeWmqFQqdGzoYFnrsnlfSyJvrRy5xK/Wv53i/5uKkpZ5B/BB4N1m9vPwzzbgL4BzzGw/cE54P3U+djFcSu+WXkZ2jvCtf/ctjkwd4dCRQ5Hrt/f8cg9T01OLPsbMWL9m/cv3k6q7V42/xK+fIMc+V3t4vNiiVMv8H+ecOefe6Jw7I/xzh3PukHOuxzm3Obx9Ls4B16vMs7w4Vy23P377MTP2hcyM41cfn3jeup4cefHPs/hZuQEfI8gCW3j7sWyH87JegpOnnQRj66QoJ1OX4m1vmTK3c41z1bLwl0QtlUolldrwxXLkxT/PsrByYzS8D8UONh8Drp9zf3rO/evSH84xein2+1udt+0HylwJEeeqZWEapJZVbasa/tlxK/55Fl8rNwZqHL8ev1Yn+eJtcIdXctAzV88wsnOkFIEd4s1Nn3/K+bTa4gu8Vmvl/FPOb/hnx63451l8rdyYXuRrs6sTBfi4eR3cyyrOVcuO03bQ2rJEcG9p5UOnfajZ4cam+OdZfK3caFni6z6sTvJHwd1Tca1aTl5zMtf+8bWsaF1xzAy+1VpZ0bqCa//4Wk5ec3Icw46k+NU0vlZu9C39kMKvTvJHwV2WdPbGs7l5+81c/PqLWd22GsNY3baai19/MTdvv5mzN56d9RCB+lcs9VTUZFN1k5fKjbgrdq4DPsriM/iir07yR5fZk1Kpp59MuXvOJN1rpdi9XPIm0d4yIkVST0VN8atuoki6Yift1Ymv+waW5m2du0g19VTUFL/qJoo0KnbSqiv3dd9AfRTcIxgcHmTX0C7GxsfoWNtBf09/CZbtxdaxtoPR8WPbS8+tqKnnMf7qIAiC1Y7HZ/zgBD+/60keve8ZJl+cpm1FC6dufS1nnHMyazfUt7diaYutQvz/f6q0TJN0/c1iqqeipvhVN1EkX7Ez+tAhbvzMfTz8s6eYfDGogZ98cZqHf/YUN37mPkYfOhTTM/m6b6A+pQnucVc/lDsvW1z1VNSUeXdz0jnx8YMT3DkwzNTRGdz0/GION+2YOjrDnQPDjB+sr+3F4nzdN1CfUlTLJFH9kPfrbyplJK8YJEhFjBEEtn6ySkv8+NuP8vDPnjomsM9lLca//qPX8ceXnhrx2fyvzCl9tUwSs+w874ZUykheMRvgRgFH1tv9H73vmUUDOwQz+F/d+0wMz5aXfQPZKEVwT6L6Ic95WaWM5BX5akY2m2NfytGX6nvc0nqBEWAmvM06sKdXmlmK4J7ELDvPedlyl/LJfPk6qdi2Yqk+M4Fly+t7XLGku4oqRXBPapad166TeU4ZSdrydVLx1K2vxVps0cdYi/H6t742pRGlKd1VVCmCe55n2UnIc8pI0pavZmRnnHMyLUsE95YW44z3ZN+ILn7prqJKEdwhv7PsJOTll1nxL3nng16C69TPpjlawvvZ/Ptfu6Gdc/u20LqscswM3lqmaF02w7l9W2LcyJQn6a6iSlEKKekrd/OtPMlnOeD4wQl+/qMhfnUvHH1pJcuWH+H1b72LM97zA9Zu+G+Zji058X8Wi5VCKrhLIrp2d1Xdwt+5tpORnSPpD6i0uqjeTqCToHokS13kd2xJiXfPwWLBXb1lJBGq2MmLfFXLzJfnsSUlvYtxlybnLulSxU41WbSfzVe1zHx5HlvxKbhLIlSxs1BWO0XzVS0zX57HVnwK7pKIvFTs5EdWO0XzvAU/z2NLSnqrN51QFUlFBao0mguCWvaN5iQN6VbLaOYeQZp13KoZLzrll/3R7Ow73dWbqmWatLCOe7bzIhB76iHN55Kk9FN91qb8crFEuXSfdqgWQpqdF6M+V5RZv1YMcSljfjmKvF7YOsrsO93Vm2buTUqzjjvKc0WZ9WvFELf0apyLLc8Xto4y+0539ZbIzN3MzjWzR83sMTO7MonnyFqaddxRnivKrD/O1UkWKwCtOooqXz3o54sy+0539RZ7cDezFuArwHuB04BLzey0uJ8na2nWcUd5riiz/rhWJ1lcGUpXoyqyPO9cjVqbn97FQ5KYuW8FHnPOPe6cOwrcCFyYwPNkKs067ijPFWXWH9fqJIsrQ+lqVEWW58qi4pw7SSLnfhLw5Jz7B4C3LnyQmfURJtI6OvLwoTWud0tvarnnZp+rv6e/anfGemb9Ub53riz6zKi3TZHlvbKoGOdOkpi5V+vEf8zuDefcgHOu2znXvWHDhgSGIRBt1h/X6iSLPjPqbVNkxZkd51nsO1TN7G3Anzvn/k14/yoA59znan2Pdqj6LYve7uonL2WQ9g7VfwQ2m9kmM1sGXALsTeB5pCCy6DOj3jZSdon0ljGzbcBugmt6fc05t2iyTDN3EZHGpX6xDufcHcAdSfxsERFZmtoPiIh4SMFdRMRDCu4iIh5ScBcR8ZCCu4iIhxTcRUQ8pOAuIuKhXFwg28wOEnTkj+rVwL/E8HOKQq/XX2V6raDX26xO51zV5ly5CO5xMbN9tXZr+Uiv119leq2g15sEpWVERDyk4C4i4iHfgvtA1gNImV6vv8r0WkGvN3Ze5dxFRCTg28xdRERQcBcR8ZI3wd3MzjWzR83sMTO7MuvxxMnMTjazu83sETN72MwuD48fb2Z3mdn+8Pa4rMcaJzNrMbMHzez28P4mM7s3fL3fDa/05QUzW2dmN5nZP4ef89t8/XzN7BPhv+OHzOw7ZrbCp8/WzL5mZs+a2UNzjlX9LC3w12Hc+oWZnRnXOLwI7mbWAnwFeC9wGnCpmZ2W7ahiNQX8V+fcG4CzgD8NX9+VwJBzbjMwFN73yeXAI3Pufx74Yvh6fwtclsmokvEl4E7n3B8CbyJ43d59vmZ2EvBnQLdz7nSCq7Vdgl+f7TeAcxccq/VZvhfYHP7pA66PaxBeBHdgK/CYc+5x59xR4EbgwozHFBvn3NPOuQfCv/+O4D/+SQSvcU/4sD3ARY7rRQMAAAJoSURBVNmMMH5mthE4D/hqeN+AdwM3hQ/x5vWa2RrgncANAM65o8655/H3820FVppZK9AOPI1Hn61z7ifAcwsO1/osLwS+6QL3AOvM7MQ4xuFLcD8JeHLO/QPhMe+YWRfwZuBe4DXOuach+AUAnJDdyGK3G/gkMBPeXw8875ybCu/79BmfAhwEvh6mob5qZqvw8PN1zv0/4K+AMYKgPg7cj7+f7axan2ViscuX4G5VjnlX42lmq4HvAzudc4ezHk9SzOx84Fnn3P1zD1d5qC+fcStwJnC9c+7NwAt4kIKpJsw1XwhsAl4HrCJITSzky2e7lMT+XfsS3A8AJ8+5vxF4KqOxJMLM2ggC+6Bz7ubw8G9ml3Dh7bNZjS9m7wAuMLMRghTbuwlm8uvCpTz49RkfAA445+4N799EEOx9/HzfAzzhnDvonJsEbgbejr+f7axan2ViscuX4P6PwObwjPsyghM0ezMeU2zCfPMNwCPOuWvnfGkvsCP8+w7g1rTHlgTn3FXOuY3OuS6Cz/IfnHO9wN3AxeHDfHq9zwBPmtmp4aEe4Jf4+fmOAWeZWXv473r2tXr52c5R67PcC3worJo5CxifTd9E5pzz4g+wDfgV8GtgV9bjifm1/RHBUu0XwM/DP9sI8tBDwP7w9visx5rAa/8T4Pbw76cA9wGPAf8TWJ71+GJ8nWcA+8LP+BbgOF8/X+Aa4J+Bh4BvAct9+myB7xCcT5gkmJlfVuuzJEjLfCWMW8MEVUSxjEPtB0REPORLWkZEROZQcBcR8ZCCu4iIhxTcRUQ8pOAuIuIhBXcREQ8puIuIeOj/A4JZ76x/eSfuAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "color = ['red', 'green', 'grey', 'black', 'yellow', 'orange']\n",
    "\n",
    "for i, c in enumerate(centers):\n",
    "    for location in centers[c]:\n",
    "        plt.scatter(*location, c=color[i])\n",
    "        \n",
    "for center in cluster.cluster_centers_:\n",
    "    plt.scatter(*center, s=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part-2 Question and Answer 问答"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1. What's the *model*? why  all the models are wrong, but some are useful? (5 points) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ans: \n",
    "  models往小了说是机器学习中的模型，往大了说可以指科学。科学只能是再做特定时间和空间下显示正确或者说预测正确，然而跳出这个时间空间就有可能是被证明错误的。比如人类历史上的日心说，地心说之间的矛盾。机器学习的模型可能被证明在当前是有用的，但是在未来某一天被证明是错误的。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<评阅点>\n",
    "> + 对模型的理解是否正确,对模型的抽象性是否正确(5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2. What's the underfitting and overfitting? List the reasons that could make model overfitting or underfitting. (10 points)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    欠拟合：模型学习不到位，在预测准确性上有很大欠缺。比如把猫和狗都当成了猫\n",
    "    过拟合：模型的泛化能力差，学习过度。比如金毛和哈士奇都是狗，模型用金毛的\n",
    "          特征来训练，结果无法判断出哈士奇是狗。\n",
    "    欠拟合产生原因：模型学习不到位，可能是样本特征数量太少或是数据量不够大导致的。\n",
    "    过拟合产生原因：1)样本的特征数量太多，而训练样本数目太少。\n",
    "              2)原始数据的噪声大\n",
    "              3)模型复杂程度过高，参数太多。 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<评阅点>\n",
    "> + 对过拟合和欠拟合的理解是否正确 (3')\n",
    "+ 对欠拟合产生的原因是否理解正确(2')\n",
    "+ 对过拟合产生的原因是否理解正确(5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3. What's the precision, recall, AUC, F1, F2score. What are they mainly target on? (12')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    Ans: precision：查准率/精度 = TP/(TP+FP)\n",
    "                    实际预测正确/所有我预测为正确的\n",
    "    recall: 查全率/召回率 = TP/(TP+FN)\n",
    "                    实际预测正确/所有实际正确的\n",
    "    AUC : 为ROC曲线与recall，precision形成的面积，AUC越大越好\n",
    "   \n",
    "    F1: 调和平均 F1 = 2*PR/P+R, 在F1中beta为1\n",
    "    F2: F1的变形 F2 = (1+beta^2)*PR/(beta^2)(P+R)， 在F2中beta大于1\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<评阅点>\n",
    "> + 对precision, recall, AUC, F1, F2 理解是否正确(6‘)\n",
    "+ 对precision, recall, AUC, F1, F2的使用侧重点是否理解正确 (6’)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4. Based on our course and yourself mind, what's the machine learning?  (8')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    传统的编程为：程序员输入指令，然后电脑去执行，所有的思维来自于程序员。\n",
    "    机器学习为：程序员训练模型，程序员输入指定到模型，由模型输出结果，中间由电脑思考。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<评阅点> 开放式问题，是否能说出来机器学习这种思维方式和传统的分析式编程的区别（8'）"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 5. \"正确定义了机器学习模型的评价标准(evaluation)， 问题基本上就已经解决一半\". 这句话是否正确？你是怎么看待的？ (8‘)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "      机器学习的最终目的是模型预测的准确性和产品落地化。那么合适的模型评价标准就在\n",
    "      调参过程中起到重要的作用。如果评价标准不合适，那么模型的表现很难有一个参照物，\n",
    "      那么调参的方向也成了问题。可以说合适的评价标准指明了优化方向。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<评阅点> 开放式问题，主要看能理解评价指标对机器学习模型的重要性."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part-03 Programming Practice 编程练习"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. In our course and previous practice, we complete some importance components of Decision Tree. In this problem, you need to build a **completed** Decision Tree Model. You show finish a `predicate()` function, which accepts three parameters **<gender, income, family_number>**, and outputs the predicated 'bought': 1 or 0.  (20 points)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# you code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<评阅点>\n",
    "> + 是否将之前的决策树模型的部分进行合并组装， predicate函数能够顺利运行(8')\n",
    "+ 是够能够输入未曾见过的X变量，例如gender, income, family_number 分别是： <M, -10, 1>, 模型能够预测出结果 (12')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. 将上一节课(第二节课)的线性回归问题中的Loss函数改成\"绝对值\"，并且改变其偏导的求值方式，观察其结果的变化。(19 point)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_boston\n",
    "import random\n",
    "dataset = load_boston()\n",
    "x,y=dataset['data'],dataset['target']\n",
    "X_rm = x[:,5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.collections.PathCollection at 0x24296970208>"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.scatter(X_rm,y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### assume that the target function is a linear function\n",
    "$$ y = k * rm + b $$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "def price(rm, k, b):\n",
    "    return k * rm + b\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### define mean absolute loss（改为绝对值)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$ (previous) MSE:loss = \\frac{1}{n} \\sum^{n}_{i=1}{(y_i - \\hat y_i)^2}$$ \n",
    "$$ (current) MAE:loss = \\frac{1}{n} \\sum^{n}_{i=1}{|y_i - \\hat y_i|} = \\frac{1}{n} \\sum^{n}_{i=1}{|y_i - (kx_i + b_i)|}$$ "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "偏导数\n",
    "$$ \\frac{\\partial loss}{\\partial k} = - \\frac{1}{n}\\sum^{n}_{i=1}|x_i|$$\n",
    "$$ \\frac{\\partial loss}{\\partial b} = -1$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "def partial_derivative_k(x): \n",
    "    gradient = -np.mean(np.abs(list(x)))\n",
    "    return gradient\n",
    "\n",
    "def partial_derivative_b():\n",
    "    return -1\n",
    "\n",
    "def loss(y,y_hat):\n",
    "    return sum((y_i - y_hat_i)**2 for y_i, y_hat_i in zip(list(y),list(y_hat)))/len(list(y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-6.284634387351779"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "partial_derivative_k(X_rm) #斜率"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 0, the loss is 240687.04368303253, parameters k is -88.04105178572186 and b is 90.04651181362559\n",
      "Iteration 1, the loss is 240647.0978659342, parameters k is -88.0347671513345 and b is 90.04751181362559\n",
      "Iteration 2, the loss is 240607.15536771, parameters k is -88.02848251694715 and b is 90.0485118136256\n",
      "Iteration 3, the loss is 240567.21618835884, parameters k is -88.0221978825598 and b is 90.0495118136256\n",
      "Iteration 4, the loss is 240527.28032788183, parameters k is -88.01591324817244 and b is 90.0505118136256\n",
      "Iteration 5, the loss is 240487.34778627782, parameters k is -88.00962861378508 and b is 90.05151181362561\n",
      "Iteration 6, the loss is 240447.41856354766, parameters k is -88.00334397939773 and b is 90.05251181362561\n",
      "Iteration 7, the loss is 240407.49265969143, parameters k is -87.99705934501037 and b is 90.05351181362562\n",
      "Iteration 8, the loss is 240367.5700747085, parameters k is -87.99077471062301 and b is 90.05451181362562\n",
      "Iteration 9, the loss is 240327.6508085991, parameters k is -87.98449007623566 and b is 90.05551181362563\n",
      "Iteration 10, the loss is 240287.73486136337, parameters k is -87.9782054418483 and b is 90.05651181362563\n",
      "Iteration 11, the loss is 240247.8222330011, parameters k is -87.97192080746095 and b is 90.05751181362564\n",
      "Iteration 12, the loss is 240207.9129235124, parameters k is -87.96563617307359 and b is 90.05851181362564\n",
      "Iteration 13, the loss is 240168.00693289755, parameters k is -87.95935153868624 and b is 90.05951181362565\n",
      "Iteration 14, the loss is 240128.10426115597, parameters k is -87.95306690429888 and b is 90.06051181362565\n",
      "Iteration 15, the loss is 240088.20490828794, parameters k is -87.94678226991152 and b is 90.06151181362566\n",
      "Iteration 16, the loss is 240048.30887429393, parameters k is -87.94049763552417 and b is 90.06251181362566\n",
      "Iteration 17, the loss is 240008.41615917315, parameters k is -87.93421300113681 and b is 90.06351181362567\n",
      "Iteration 18, the loss is 239968.52676292625, parameters k is -87.92792836674946 and b is 90.06451181362567\n",
      "Iteration 19, the loss is 239928.64068555256, parameters k is -87.9216437323621 and b is 90.06551181362568\n",
      "Iteration 20, the loss is 239888.75792705253, parameters k is -87.91535909797474 and b is 90.06651181362568\n",
      "Iteration 21, the loss is 239848.87848742647, parameters k is -87.90907446358739 and b is 90.06751181362569\n",
      "Iteration 22, the loss is 239809.00236667346, parameters k is -87.90278982920003 and b is 90.06851181362569\n",
      "Iteration 23, the loss is 239769.1295647944, parameters k is -87.89650519481268 and b is 90.0695118136257\n",
      "Iteration 24, the loss is 239729.2600817887, parameters k is -87.89022056042532 and b is 90.0705118136257\n",
      "Iteration 25, the loss is 239689.3939176565, parameters k is -87.88393592603796 and b is 90.0715118136257\n",
      "Iteration 26, the loss is 239649.53107239836, parameters k is -87.87765129165061 and b is 90.07251181362571\n",
      "Iteration 27, the loss is 239609.67154601336, parameters k is -87.87136665726325 and b is 90.07351181362571\n",
      "Iteration 28, the loss is 239569.81533850209, parameters k is -87.8650820228759 and b is 90.07451181362572\n",
      "Iteration 29, the loss is 239529.96244986443, parameters k is -87.85879738848854 and b is 90.07551181362572\n",
      "Iteration 30, the loss is 239490.11288010026, parameters k is -87.85251275410118 and b is 90.07651181362573\n",
      "Iteration 31, the loss is 239450.2666292097, parameters k is -87.84622811971383 and b is 90.07751181362573\n",
      "Iteration 32, the loss is 239410.42369719292, parameters k is -87.83994348532647 and b is 90.07851181362574\n",
      "Iteration 33, the loss is 239370.58408404927, parameters k is -87.83365885093912 and b is 90.07951181362574\n",
      "Iteration 34, the loss is 239330.74778977997, parameters k is -87.82737421655176 and b is 90.08051181362575\n",
      "Iteration 35, the loss is 239290.91481438355, parameters k is -87.8210895821644 and b is 90.08151181362575\n",
      "Iteration 36, the loss is 239251.08515786097, parameters k is -87.81480494777705 and b is 90.08251181362576\n",
      "Iteration 37, the loss is 239211.2588202119, parameters k is -87.80852031338969 and b is 90.08351181362576\n",
      "Iteration 38, the loss is 239171.43580143625, parameters k is -87.80223567900234 and b is 90.08451181362577\n",
      "Iteration 39, the loss is 239131.61610153457, parameters k is -87.79595104461498 and b is 90.08551181362577\n",
      "Iteration 40, the loss is 239091.79972050624, parameters k is -87.78966641022762 and b is 90.08651181362578\n",
      "Iteration 41, the loss is 239051.98665835164, parameters k is -87.78338177584027 and b is 90.08751181362578\n",
      "Iteration 42, the loss is 239012.17691507054, parameters k is -87.77709714145291 and b is 90.08851181362579\n",
      "Iteration 43, the loss is 238972.37049066316, parameters k is -87.77081250706556 and b is 90.08951181362579\n",
      "Iteration 44, the loss is 238932.56738512905, parameters k is -87.7645278726782 and b is 90.0905118136258\n",
      "Iteration 45, the loss is 238892.76759846872, parameters k is -87.75824323829085 and b is 90.0915118136258\n",
      "Iteration 46, the loss is 238852.9711306819, parameters k is -87.75195860390349 and b is 90.0925118136258\n",
      "Iteration 47, the loss is 238813.1779817689, parameters k is -87.74567396951613 and b is 90.09351181362581\n",
      "Iteration 48, the loss is 238773.3881517292, parameters k is -87.73938933512878 and b is 90.09451181362581\n",
      "Iteration 49, the loss is 238733.60164056317, parameters k is -87.73310470074142 and b is 90.09551181362582\n",
      "Iteration 50, the loss is 238693.81844827087, parameters k is -87.72682006635407 and b is 90.09651181362582\n",
      "Iteration 51, the loss is 238654.03857485164, parameters k is -87.72053543196671 and b is 90.09751181362583\n",
      "Iteration 52, the loss is 238614.26202030672, parameters k is -87.71425079757935 and b is 90.09851181362583\n",
      "Iteration 53, the loss is 238574.4887846347, parameters k is -87.707966163192 and b is 90.09951181362584\n",
      "Iteration 54, the loss is 238534.71886783658, parameters k is -87.70168152880464 and b is 90.10051181362584\n",
      "Iteration 55, the loss is 238494.9522699123, parameters k is -87.69539689441729 and b is 90.10151181362585\n",
      "Iteration 56, the loss is 238455.1889908614, parameters k is -87.68911226002993 and b is 90.10251181362585\n",
      "Iteration 57, the loss is 238415.42903068397, parameters k is -87.68282762564257 and b is 90.10351181362586\n",
      "Iteration 58, the loss is 238375.67238938008, parameters k is -87.67654299125522 and b is 90.10451181362586\n",
      "Iteration 59, the loss is 238335.91906695027, parameters k is -87.67025835686786 and b is 90.10551181362587\n",
      "Iteration 60, the loss is 238296.16906339335, parameters k is -87.6639737224805 and b is 90.10651181362587\n",
      "Iteration 61, the loss is 238256.4223787105, parameters k is -87.65768908809315 and b is 90.10751181362588\n",
      "Iteration 62, the loss is 238216.6790129011, parameters k is -87.6514044537058 and b is 90.10851181362588\n",
      "Iteration 63, the loss is 238176.93896596512, parameters k is -87.64511981931844 and b is 90.10951181362589\n",
      "Iteration 64, the loss is 238137.2022379029, parameters k is -87.63883518493108 and b is 90.11051181362589\n",
      "Iteration 65, the loss is 238097.46882871402, parameters k is -87.63255055054373 and b is 90.1115118136259\n",
      "Iteration 66, the loss is 238057.73873839906, parameters k is -87.62626591615637 and b is 90.1125118136259\n",
      "Iteration 67, the loss is 238018.01196695762, parameters k is -87.61998128176901 and b is 90.1135118136259\n",
      "Iteration 68, the loss is 237978.28851438966, parameters k is -87.61369664738166 and b is 90.11451181362591\n",
      "Iteration 69, the loss is 237938.56838069513, parameters k is -87.6074120129943 and b is 90.11551181362591\n",
      "Iteration 70, the loss is 237898.85156587465, parameters k is -87.60112737860695 and b is 90.11651181362592\n",
      "Iteration 71, the loss is 237859.1380699274, parameters k is -87.59484274421959 and b is 90.11751181362592\n",
      "Iteration 72, the loss is 237819.4278928539, parameters k is -87.58855810983223 and b is 90.11851181362593\n",
      "Iteration 73, the loss is 237779.7210346538, parameters k is -87.58227347544488 and b is 90.11951181362593\n",
      "Iteration 74, the loss is 237740.01749532716, parameters k is -87.57598884105752 and b is 90.12051181362594\n",
      "Iteration 75, the loss is 237700.31727487443, parameters k is -87.56970420667017 and b is 90.12151181362594\n",
      "Iteration 76, the loss is 237660.6203732951, parameters k is -87.56341957228281 and b is 90.12251181362595\n",
      "Iteration 77, the loss is 237620.92679058967, parameters k is -87.55713493789546 and b is 90.12351181362595\n",
      "Iteration 78, the loss is 237581.2365267575, parameters k is -87.5508503035081 and b is 90.12451181362596\n",
      "Iteration 79, the loss is 237541.54958179893, parameters k is -87.54456566912074 and b is 90.12551181362596\n",
      "Iteration 80, the loss is 237501.86595571408, parameters k is -87.53828103473339 and b is 90.12651181362597\n",
      "Iteration 81, the loss is 237462.18564850278, parameters k is -87.53199640034603 and b is 90.12751181362597\n",
      "Iteration 82, the loss is 237422.5086601651, parameters k is -87.52571176595868 and b is 90.12851181362598\n",
      "Iteration 83, the loss is 237382.8349907008, parameters k is -87.51942713157132 and b is 90.12951181362598\n",
      "Iteration 84, the loss is 237343.16464010996, parameters k is -87.51314249718396 and b is 90.13051181362599\n",
      "Iteration 85, the loss is 237303.4976083932, parameters k is -87.50685786279661 and b is 90.13151181362599\n",
      "Iteration 86, the loss is 237263.83389554956, parameters k is -87.50057322840925 and b is 90.132511813626\n",
      "Iteration 87, the loss is 237224.17350157976, parameters k is -87.4942885940219 and b is 90.133511813626\n",
      "Iteration 88, the loss is 237184.51642648343, parameters k is -87.48800395963454 and b is 90.134511813626\n",
      "Iteration 89, the loss is 237144.86267026083, parameters k is -87.48171932524718 and b is 90.13551181362601\n",
      "Iteration 90, the loss is 237105.21223291155, parameters k is -87.47543469085983 and b is 90.13651181362602\n",
      "Iteration 91, the loss is 237065.56511443612, parameters k is -87.46915005647247 and b is 90.13751181362602\n",
      "Iteration 92, the loss is 237025.92131483392, parameters k is -87.46286542208512 and b is 90.13851181362602\n",
      "Iteration 93, the loss is 236986.280834106, parameters k is -87.45658078769776 and b is 90.13951181362603\n",
      "Iteration 94, the loss is 236946.64367225094, parameters k is -87.4502961533104 and b is 90.14051181362603\n",
      "Iteration 95, the loss is 236907.0098292696, parameters k is -87.44401151892305 and b is 90.14151181362604\n",
      "Iteration 96, the loss is 236867.37930516232, parameters k is -87.43772688453569 and b is 90.14251181362604\n",
      "Iteration 97, the loss is 236827.75209992833, parameters k is -87.43144225014834 and b is 90.14351181362605\n",
      "Iteration 98, the loss is 236788.12821356795, parameters k is -87.42515761576098 and b is 90.14451181362605\n",
      "Iteration 99, the loss is 236748.50764608092, parameters k is -87.41887298137362 and b is 90.14551181362606\n",
      "Iteration 100, the loss is 236708.89039746765, parameters k is -87.41258834698627 and b is 90.14651181362606\n",
      "Iteration 101, the loss is 236669.27646772828, parameters k is -87.40630371259891 and b is 90.14751181362607\n",
      "Iteration 102, the loss is 236629.6658568619, parameters k is -87.40001907821156 and b is 90.14851181362607\n",
      "Iteration 103, the loss is 236590.05856486957, parameters k is -87.3937344438242 and b is 90.14951181362608\n",
      "Iteration 104, the loss is 236550.45459175072, parameters k is -87.38744980943684 and b is 90.15051181362608\n",
      "Iteration 105, the loss is 236510.85393750513, parameters k is -87.38116517504949 and b is 90.15151181362609\n",
      "Iteration 106, the loss is 236471.25660213357, parameters k is -87.37488054066213 and b is 90.15251181362609\n",
      "Iteration 107, the loss is 236431.66258563532, parameters k is -87.36859590627478 and b is 90.1535118136261\n",
      "Iteration 108, the loss is 236392.0718880108, parameters k is -87.36231127188742 and b is 90.1545118136261\n",
      "Iteration 109, the loss is 236352.48450925996, parameters k is -87.35602663750007 and b is 90.1555118136261\n",
      "Iteration 110, the loss is 236312.90044938232, parameters k is -87.34974200311271 and b is 90.15651181362611\n",
      "Iteration 111, the loss is 236273.31970837852, parameters k is -87.34345736872535 and b is 90.15751181362612\n",
      "Iteration 112, the loss is 236233.7422862483, parameters k is -87.337172734338 and b is 90.15851181362612\n",
      "Iteration 113, the loss is 236194.16818299133, parameters k is -87.33088809995064 and b is 90.15951181362612\n",
      "Iteration 114, the loss is 236154.5973986085, parameters k is -87.32460346556329 and b is 90.16051181362613\n",
      "Iteration 115, the loss is 236115.02993309902, parameters k is -87.31831883117593 and b is 90.16151181362613\n",
      "Iteration 116, the loss is 236075.46578646306, parameters k is -87.31203419678857 and b is 90.16251181362614\n",
      "Iteration 117, the loss is 236035.90495870044, parameters k is -87.30574956240122 and b is 90.16351181362614\n",
      "Iteration 118, the loss is 235996.34744981176, parameters k is -87.29946492801386 and b is 90.16451181362615\n",
      "Iteration 119, the loss is 235956.79325979663, parameters k is -87.2931802936265 and b is 90.16551181362615\n",
      "Iteration 120, the loss is 235917.24238865497, parameters k is -87.28689565923915 and b is 90.16651181362616\n",
      "Iteration 121, the loss is 235877.6948363869, parameters k is -87.2806110248518 and b is 90.16751181362616\n",
      "Iteration 122, the loss is 235838.1506029924, parameters k is -87.27432639046444 and b is 90.16851181362617\n",
      "Iteration 123, the loss is 235798.60968847168, parameters k is -87.26804175607708 and b is 90.16951181362617\n",
      "Iteration 124, the loss is 235759.07209282447, parameters k is -87.26175712168973 and b is 90.17051181362618\n",
      "Iteration 125, the loss is 235719.53781605067, parameters k is -87.25547248730237 and b is 90.17151181362618\n",
      "Iteration 126, the loss is 235680.00685815036, parameters k is -87.24918785291501 and b is 90.17251181362619\n",
      "Iteration 127, the loss is 235640.47921912436, parameters k is -87.24290321852766 and b is 90.17351181362619\n",
      "Iteration 128, the loss is 235600.95489897125, parameters k is -87.2366185841403 and b is 90.1745118136262\n",
      "Iteration 129, the loss is 235561.43389769216, parameters k is -87.23033394975295 and b is 90.1755118136262\n",
      "Iteration 130, the loss is 235521.9162152864, parameters k is -87.22404931536559 and b is 90.1765118136262\n",
      "Iteration 131, the loss is 235482.40185175408, parameters k is -87.21776468097823 and b is 90.17751181362621\n",
      "Iteration 132, the loss is 235442.89080709545, parameters k is -87.21148004659088 and b is 90.17851181362622\n",
      "Iteration 133, the loss is 235403.38308131057, parameters k is -87.20519541220352 and b is 90.17951181362622\n",
      "Iteration 134, the loss is 235363.87867439873, parameters k is -87.19891077781617 and b is 90.18051181362623\n",
      "Iteration 135, the loss is 235324.37758636105, parameters k is -87.19262614342881 and b is 90.18151181362623\n",
      "Iteration 136, the loss is 235284.87981719663, parameters k is -87.18634150904145 and b is 90.18251181362623\n",
      "Iteration 137, the loss is 235245.3853669061, parameters k is -87.1800568746541 and b is 90.18351181362624\n",
      "Iteration 138, the loss is 235205.89423548905, parameters k is -87.17377224026674 and b is 90.18451181362624\n",
      "Iteration 139, the loss is 235166.4064229455, parameters k is -87.16748760587939 and b is 90.18551181362625\n",
      "Iteration 140, the loss is 235126.92192927538, parameters k is -87.16120297149203 and b is 90.18651181362625\n",
      "Iteration 141, the loss is 235087.44075447944, parameters k is -87.15491833710468 and b is 90.18751181362626\n",
      "Iteration 142, the loss is 235047.96289855664, parameters k is -87.14863370271732 and b is 90.18851181362626\n",
      "Iteration 143, the loss is 235008.48836150716, parameters k is -87.14234906832996 and b is 90.18951181362627\n",
      "Iteration 144, the loss is 234969.01714333173, parameters k is -87.13606443394261 and b is 90.19051181362627\n",
      "Iteration 145, the loss is 234929.54924402962, parameters k is -87.12977979955525 and b is 90.19151181362628\n",
      "Iteration 146, the loss is 234890.08466360127, parameters k is -87.1234951651679 and b is 90.19251181362628\n",
      "Iteration 147, the loss is 234850.6234020465, parameters k is -87.11721053078054 and b is 90.19351181362629\n",
      "Iteration 148, the loss is 234811.1654593652, parameters k is -87.11092589639318 and b is 90.19451181362629\n",
      "Iteration 149, the loss is 234771.71083555752, parameters k is -87.10464126200583 and b is 90.1955118136263\n",
      "Iteration 150, the loss is 234732.2595306234, parameters k is -87.09835662761847 and b is 90.1965118136263\n",
      "Iteration 151, the loss is 234692.8115445628, parameters k is -87.09207199323112 and b is 90.1975118136263\n",
      "Iteration 152, the loss is 234653.36687737593, parameters k is -87.08578735884376 and b is 90.19851181362631\n",
      "Iteration 153, the loss is 234613.9255290624, parameters k is -87.0795027244564 and b is 90.19951181362632\n",
      "Iteration 154, the loss is 234574.48749962274, parameters k is -87.07321809006905 and b is 90.20051181362632\n",
      "Iteration 155, the loss is 234535.05278905662, parameters k is -87.06693345568169 and b is 90.20151181362633\n",
      "Iteration 156, the loss is 234495.62139736427, parameters k is -87.06064882129434 and b is 90.20251181362633\n",
      "Iteration 157, the loss is 234456.19332454517, parameters k is -87.05436418690698 and b is 90.20351181362633\n",
      "Iteration 158, the loss is 234416.76857059967, parameters k is -87.04807955251962 and b is 90.20451181362634\n",
      "Iteration 159, the loss is 234377.3471355279, parameters k is -87.04179491813227 and b is 90.20551181362634\n",
      "Iteration 160, the loss is 234337.9290193296, parameters k is -87.03551028374491 and b is 90.20651181362635\n",
      "Iteration 161, the loss is 234298.51422200492, parameters k is -87.02922564935756 and b is 90.20751181362635\n",
      "Iteration 162, the loss is 234259.1027435538, parameters k is -87.0229410149702 and b is 90.20851181362636\n",
      "Iteration 163, the loss is 234219.69458397644, parameters k is -87.01665638058284 and b is 90.20951181362636\n",
      "Iteration 164, the loss is 234180.2897432724, parameters k is -87.01037174619549 and b is 90.21051181362637\n",
      "Iteration 165, the loss is 234140.8882214419, parameters k is -87.00408711180813 and b is 90.21151181362637\n",
      "Iteration 166, the loss is 234101.49001848514, parameters k is -86.99780247742078 and b is 90.21251181362638\n",
      "Iteration 167, the loss is 234062.09513440204, parameters k is -86.99151784303342 and b is 90.21351181362638\n",
      "Iteration 168, the loss is 234022.70356919258, parameters k is -86.98523320864606 and b is 90.21451181362639\n",
      "Iteration 169, the loss is 233983.3153228567, parameters k is -86.97894857425871 and b is 90.21551181362639\n",
      "Iteration 170, the loss is 233943.93039539413, parameters k is -86.97266393987135 and b is 90.2165118136264\n",
      "Iteration 171, the loss is 233904.5487868054, parameters k is -86.966379305484 and b is 90.2175118136264\n",
      "Iteration 172, the loss is 233865.17049709018, parameters k is -86.96009467109664 and b is 90.2185118136264\n",
      "Iteration 173, the loss is 233825.7955262483, parameters k is -86.95381003670929 and b is 90.21951181362641\n",
      "Iteration 174, the loss is 233786.42387428004, parameters k is -86.94752540232193 and b is 90.22051181362642\n",
      "Iteration 175, the loss is 233747.05554118595, parameters k is -86.94124076793457 and b is 90.22151181362642\n",
      "Iteration 176, the loss is 233707.6905269648, parameters k is -86.93495613354722 and b is 90.22251181362643\n",
      "Iteration 177, the loss is 233668.32883161772, parameters k is -86.92867149915986 and b is 90.22351181362643\n",
      "Iteration 178, the loss is 233628.97045514346, parameters k is -86.9223868647725 and b is 90.22451181362644\n",
      "Iteration 179, the loss is 233589.6153975435, parameters k is -86.91610223038515 and b is 90.22551181362644\n",
      "Iteration 180, the loss is 233550.2636588169, parameters k is -86.9098175959978 and b is 90.22651181362644\n",
      "Iteration 181, the loss is 233510.91523896402, parameters k is -86.90353296161044 and b is 90.22751181362645\n",
      "Iteration 182, the loss is 233471.57013798476, parameters k is -86.89724832722308 and b is 90.22851181362645\n",
      "Iteration 183, the loss is 233432.22835587867, parameters k is -86.89096369283573 and b is 90.22951181362646\n",
      "Iteration 184, the loss is 233392.88989264664, parameters k is -86.88467905844837 and b is 90.23051181362646\n",
      "Iteration 185, the loss is 233353.55474828777, parameters k is -86.87839442406101 and b is 90.23151181362647\n",
      "Iteration 186, the loss is 233314.22292280247, parameters k is -86.87210978967366 and b is 90.23251181362647\n",
      "Iteration 187, the loss is 233274.89441619083, parameters k is -86.8658251552863 and b is 90.23351181362648\n",
      "Iteration 188, the loss is 233235.56922845318, parameters k is -86.85954052089895 and b is 90.23451181362648\n",
      "Iteration 189, the loss is 233196.24735958886, parameters k is -86.85325588651159 and b is 90.23551181362649\n",
      "Iteration 190, the loss is 233156.9288095981, parameters k is -86.84697125212423 and b is 90.23651181362649\n",
      "Iteration 191, the loss is 233117.61357848096, parameters k is -86.84068661773688 and b is 90.2375118136265\n",
      "Iteration 192, the loss is 233078.30166623736, parameters k is -86.83440198334952 and b is 90.2385118136265\n",
      "Iteration 193, the loss is 233038.9930728673, parameters k is -86.82811734896217 and b is 90.2395118136265\n",
      "Iteration 194, the loss is 232999.68779837096, parameters k is -86.82183271457481 and b is 90.24051181362651\n",
      "Iteration 195, the loss is 232960.3858427478, parameters k is -86.81554808018745 and b is 90.24151181362652\n",
      "Iteration 196, the loss is 232921.0872059987, parameters k is -86.8092634458001 and b is 90.24251181362652\n",
      "Iteration 197, the loss is 232881.791888123, parameters k is -86.80297881141274 and b is 90.24351181362653\n",
      "Iteration 198, the loss is 232842.49988912084, parameters k is -86.79669417702539 and b is 90.24451181362653\n",
      "Iteration 199, the loss is 232803.21120899278, parameters k is -86.79040954263803 and b is 90.24551181362654\n",
      "Iteration 200, the loss is 232763.92584773788, parameters k is -86.78412490825067 and b is 90.24651181362654\n",
      "Iteration 201, the loss is 232724.6438053561, parameters k is -86.77784027386332 and b is 90.24751181362655\n",
      "Iteration 202, the loss is 232685.36508184834, parameters k is -86.77155563947596 and b is 90.24851181362655\n",
      "Iteration 203, the loss is 232646.0896772146, parameters k is -86.76527100508861 and b is 90.24951181362655\n",
      "Iteration 204, the loss is 232606.81759145384, parameters k is -86.75898637070125 and b is 90.25051181362656\n",
      "Iteration 205, the loss is 232567.54882456668, parameters k is -86.7527017363139 and b is 90.25151181362656\n",
      "Iteration 206, the loss is 232528.2833765533, parameters k is -86.74641710192654 and b is 90.25251181362657\n",
      "Iteration 207, the loss is 232489.02124741324, parameters k is -86.74013246753918 and b is 90.25351181362657\n",
      "Iteration 208, the loss is 232449.76243714732, parameters k is -86.73384783315183 and b is 90.25451181362658\n",
      "Iteration 209, the loss is 232410.50694575463, parameters k is -86.72756319876447 and b is 90.25551181362658\n",
      "Iteration 210, the loss is 232371.2547732353, parameters k is -86.72127856437712 and b is 90.25651181362659\n",
      "Iteration 211, the loss is 232332.00591958984, parameters k is -86.71499392998976 and b is 90.25751181362659\n",
      "Iteration 212, the loss is 232292.76038481804, parameters k is -86.7087092956024 and b is 90.2585118136266\n",
      "Iteration 213, the loss is 232253.51816891957, parameters k is -86.70242466121505 and b is 90.2595118136266\n",
      "Iteration 214, the loss is 232214.27927189512, parameters k is -86.69614002682769 and b is 90.26051181362661\n",
      "Iteration 215, the loss is 232175.04369374373, parameters k is -86.68985539244034 and b is 90.26151181362661\n",
      "Iteration 216, the loss is 232135.8114344663, parameters k is -86.68357075805298 and b is 90.26251181362662\n",
      "Iteration 217, the loss is 232096.58249406208, parameters k is -86.67728612366562 and b is 90.26351181362662\n",
      "Iteration 218, the loss is 232057.35687253167, parameters k is -86.67100148927827 and b is 90.26451181362663\n",
      "Iteration 219, the loss is 232018.13456987482, parameters k is -86.66471685489091 and b is 90.26551181362663\n",
      "Iteration 220, the loss is 231978.9155860915, parameters k is -86.65843222050356 and b is 90.26651181362664\n",
      "Iteration 221, the loss is 231939.69992118207, parameters k is -86.6521475861162 and b is 90.26751181362664\n",
      "Iteration 222, the loss is 231900.48757514602, parameters k is -86.64586295172884 and b is 90.26851181362665\n",
      "Iteration 223, the loss is 231861.27854798347, parameters k is -86.63957831734149 and b is 90.26951181362665\n",
      "Iteration 224, the loss is 231822.0728396946, parameters k is -86.63329368295413 and b is 90.27051181362665\n",
      "Iteration 225, the loss is 231782.8704502792, parameters k is -86.62700904856678 and b is 90.27151181362666\n",
      "Iteration 226, the loss is 231743.67137973764, parameters k is -86.62072441417942 and b is 90.27251181362666\n",
      "Iteration 227, the loss is 231704.4756280693, parameters k is -86.61443977979206 and b is 90.27351181362667\n",
      "Iteration 228, the loss is 231665.28319527433, parameters k is -86.60815514540471 and b is 90.27451181362667\n",
      "Iteration 229, the loss is 231626.09408135372, parameters k is -86.60187051101735 and b is 90.27551181362668\n",
      "Iteration 230, the loss is 231586.9082863064, parameters k is -86.59558587663 and b is 90.27651181362668\n",
      "Iteration 231, the loss is 231547.72581013228, parameters k is -86.58930124224264 and b is 90.27751181362669\n",
      "Iteration 232, the loss is 231508.54665283216, parameters k is -86.58301660785528 and b is 90.2785118136267\n",
      "Iteration 233, the loss is 231469.37081440559, parameters k is -86.57673197346793 and b is 90.2795118136267\n",
      "Iteration 234, the loss is 231430.1982948524, parameters k is -86.57044733908057 and b is 90.2805118136267\n",
      "Iteration 235, the loss is 231391.02909417296, parameters k is -86.56416270469322 and b is 90.28151181362671\n",
      "Iteration 236, the loss is 231351.86321236735, parameters k is -86.55787807030586 and b is 90.28251181362671\n",
      "Iteration 237, the loss is 231312.70064943458, parameters k is -86.5515934359185 and b is 90.28351181362672\n",
      "Iteration 238, the loss is 231273.54140537596, parameters k is -86.54530880153115 and b is 90.28451181362672\n",
      "Iteration 239, the loss is 231234.38548019077, parameters k is -86.5390241671438 and b is 90.28551181362673\n",
      "Iteration 240, the loss is 231195.23287387923, parameters k is -86.53273953275644 and b is 90.28651181362673\n",
      "Iteration 241, the loss is 231156.08358644115, parameters k is -86.52645489836908 and b is 90.28751181362674\n",
      "Iteration 242, the loss is 231116.93761787674, parameters k is -86.52017026398173 and b is 90.28851181362674\n",
      "Iteration 243, the loss is 231077.7949681859, parameters k is -86.51388562959437 and b is 90.28951181362675\n",
      "Iteration 244, the loss is 231038.65563736862, parameters k is -86.50760099520701 and b is 90.29051181362675\n",
      "Iteration 245, the loss is 230999.51962542502, parameters k is -86.50131636081966 and b is 90.29151181362676\n",
      "Iteration 246, the loss is 230960.38693235486, parameters k is -86.4950317264323 and b is 90.29251181362676\n",
      "Iteration 247, the loss is 230921.25755815845, parameters k is -86.48874709204495 and b is 90.29351181362676\n",
      "Iteration 248, the loss is 230882.13150283523, parameters k is -86.48246245765759 and b is 90.29451181362677\n",
      "Iteration 249, the loss is 230843.00876638584, parameters k is -86.47617782327023 and b is 90.29551181362677\n",
      "Iteration 250, the loss is 230803.8893488101, parameters k is -86.46989318888288 and b is 90.29651181362678\n",
      "Iteration 251, the loss is 230764.77325010803, parameters k is -86.46360855449552 and b is 90.29751181362678\n",
      "Iteration 252, the loss is 230725.66047027946, parameters k is -86.45732392010817 and b is 90.29851181362679\n",
      "Iteration 253, the loss is 230686.5510093247, parameters k is -86.45103928572081 and b is 90.2995118136268\n",
      "Iteration 254, the loss is 230647.44486724303, parameters k is -86.44475465133345 and b is 90.3005118136268\n",
      "Iteration 255, the loss is 230608.3420440355, parameters k is -86.4384700169461 and b is 90.3015118136268\n",
      "Iteration 256, the loss is 230569.24253970126, parameters k is -86.43218538255874 and b is 90.30251181362681\n",
      "Iteration 257, the loss is 230530.14635424022, parameters k is -86.42590074817139 and b is 90.30351181362681\n",
      "Iteration 258, the loss is 230491.05348765338, parameters k is -86.41961611378403 and b is 90.30451181362682\n",
      "Iteration 259, the loss is 230451.96393993977, parameters k is -86.41333147939667 and b is 90.30551181362682\n",
      "Iteration 260, the loss is 230412.87771109986, parameters k is -86.40704684500932 and b is 90.30651181362683\n",
      "Iteration 261, the loss is 230373.79480113374, parameters k is -86.40076221062196 and b is 90.30751181362683\n",
      "Iteration 262, the loss is 230334.71521004065, parameters k is -86.3944775762346 and b is 90.30851181362684\n",
      "Iteration 263, the loss is 230295.63893782173, parameters k is -86.38819294184725 and b is 90.30951181362684\n",
      "Iteration 264, the loss is 230256.5659844762, parameters k is -86.3819083074599 and b is 90.31051181362685\n",
      "Iteration 265, the loss is 230217.49635000413, parameters k is -86.37562367307254 and b is 90.31151181362685\n",
      "Iteration 266, the loss is 230178.4300344057, parameters k is -86.36933903868518 and b is 90.31251181362686\n",
      "Iteration 267, the loss is 230139.3670376807, parameters k is -86.36305440429783 and b is 90.31351181362686\n",
      "Iteration 268, the loss is 230100.30735982955, parameters k is -86.35676976991047 and b is 90.31451181362686\n",
      "Iteration 269, the loss is 230061.25100085186, parameters k is -86.35048513552312 and b is 90.31551181362687\n",
      "Iteration 270, the loss is 230022.19796074778, parameters k is -86.34420050113576 and b is 90.31651181362687\n",
      "Iteration 271, the loss is 229983.14823951726, parameters k is -86.3379158667484 and b is 90.31751181362688\n",
      "Iteration 272, the loss is 229944.1018371606, parameters k is -86.33163123236105 and b is 90.31851181362688\n",
      "Iteration 273, the loss is 229905.05875367715, parameters k is -86.32534659797369 and b is 90.31951181362689\n",
      "Iteration 274, the loss is 229866.01898906758, parameters k is -86.31906196358634 and b is 90.3205118136269\n",
      "Iteration 275, the loss is 229826.9825433314, parameters k is -86.31277732919898 and b is 90.3215118136269\n",
      "Iteration 276, the loss is 229787.9494164688, parameters k is -86.30649269481162 and b is 90.3225118136269\n",
      "Iteration 277, the loss is 229748.91960847966, parameters k is -86.30020806042427 and b is 90.32351181362691\n",
      "Iteration 278, the loss is 229709.8931193641, parameters k is -86.29392342603691 and b is 90.32451181362691\n",
      "Iteration 279, the loss is 229670.86994912266, parameters k is -86.28763879164956 and b is 90.32551181362692\n",
      "Iteration 280, the loss is 229631.85009775416, parameters k is -86.2813541572622 and b is 90.32651181362692\n",
      "Iteration 281, the loss is 229592.83356525953, parameters k is -86.27506952287484 and b is 90.32751181362693\n",
      "Iteration 282, the loss is 229553.8203516383, parameters k is -86.26878488848749 and b is 90.32851181362693\n",
      "Iteration 283, the loss is 229514.81045689073, parameters k is -86.26250025410013 and b is 90.32951181362694\n",
      "Iteration 284, the loss is 229475.80388101697, parameters k is -86.25621561971278 and b is 90.33051181362694\n",
      "Iteration 285, the loss is 229436.80062401664, parameters k is -86.24993098532542 and b is 90.33151181362695\n",
      "Iteration 286, the loss is 229397.80068589, parameters k is -86.24364635093806 and b is 90.33251181362695\n",
      "Iteration 287, the loss is 229358.80406663657, parameters k is -86.23736171655071 and b is 90.33351181362696\n",
      "Iteration 288, the loss is 229319.81076625714, parameters k is -86.23107708216335 and b is 90.33451181362696\n",
      "Iteration 289, the loss is 229280.8207847509, parameters k is -86.224792447776 and b is 90.33551181362697\n",
      "Iteration 290, the loss is 229241.83412211886, parameters k is -86.21850781338864 and b is 90.33651181362697\n",
      "Iteration 291, the loss is 229202.85077835954, parameters k is -86.21222317900128 and b is 90.33751181362697\n",
      "Iteration 292, the loss is 229163.87075347482, parameters k is -86.20593854461393 and b is 90.33851181362698\n",
      "Iteration 293, the loss is 229124.89404746282, parameters k is -86.19965391022657 and b is 90.33951181362698\n",
      "Iteration 294, the loss is 229085.920660325, parameters k is -86.19336927583922 and b is 90.34051181362699\n",
      "Iteration 295, the loss is 229046.9505920603, parameters k is -86.18708464145186 and b is 90.341511813627\n",
      "Iteration 296, the loss is 229007.98384266932, parameters k is -86.1808000070645 and b is 90.342511813627\n",
      "Iteration 297, the loss is 228969.02041215202, parameters k is -86.17451537267715 and b is 90.343511813627\n",
      "Iteration 298, the loss is 228930.0603005086, parameters k is -86.1682307382898 and b is 90.34451181362701\n",
      "Iteration 299, the loss is 228891.10350773836, parameters k is -86.16194610390244 and b is 90.34551181362701\n",
      "Iteration 300, the loss is 228852.15003384167, parameters k is -86.15566146951508 and b is 90.34651181362702\n",
      "Iteration 301, the loss is 228813.19987881894, parameters k is -86.14937683512773 and b is 90.34751181362702\n",
      "Iteration 302, the loss is 228774.25304266918, parameters k is -86.14309220074037 and b is 90.34851181362703\n",
      "Iteration 303, the loss is 228735.30952539327, parameters k is -86.13680756635301 and b is 90.34951181362703\n",
      "Iteration 304, the loss is 228696.36932699115, parameters k is -86.13052293196566 and b is 90.35051181362704\n",
      "Iteration 305, the loss is 228657.43244746258, parameters k is -86.1242382975783 and b is 90.35151181362704\n",
      "Iteration 306, the loss is 228618.49888680747, parameters k is -86.11795366319095 and b is 90.35251181362705\n",
      "Iteration 307, the loss is 228579.5686450259, parameters k is -86.11166902880359 and b is 90.35351181362705\n",
      "Iteration 308, the loss is 228540.64172211805, parameters k is -86.10538439441623 and b is 90.35451181362706\n",
      "Iteration 309, the loss is 228501.71811808352, parameters k is -86.09909976002888 and b is 90.35551181362706\n",
      "Iteration 310, the loss is 228462.79783292278, parameters k is -86.09281512564152 and b is 90.35651181362707\n",
      "Iteration 311, the loss is 228423.88086663577, parameters k is -86.08653049125417 and b is 90.35751181362707\n",
      "Iteration 312, the loss is 228384.96721922225, parameters k is -86.08024585686681 and b is 90.35851181362708\n",
      "Iteration 313, the loss is 228346.05689068194, parameters k is -86.07396122247945 and b is 90.35951181362708\n",
      "Iteration 314, the loss is 228307.14988101594, parameters k is -86.0676765880921 and b is 90.36051181362708\n",
      "Iteration 315, the loss is 228268.2461902228, parameters k is -86.06139195370474 and b is 90.36151181362709\n",
      "Iteration 316, the loss is 228229.3458183039, parameters k is -86.05510731931739 and b is 90.3625118136271\n",
      "Iteration 317, the loss is 228190.44876525836, parameters k is -86.04882268493003 and b is 90.3635118136271\n",
      "Iteration 318, the loss is 228151.55503108588, parameters k is -86.04253805054267 and b is 90.3645118136271\n",
      "Iteration 319, the loss is 228112.66461578745, parameters k is -86.03625341615532 and b is 90.36551181362711\n",
      "Iteration 320, the loss is 228073.77751936243, parameters k is -86.02996878176796 and b is 90.36651181362711\n",
      "Iteration 321, the loss is 228034.8937418111, parameters k is -86.0236841473806 and b is 90.36751181362712\n",
      "Iteration 322, the loss is 227996.0132831334, parameters k is -86.01739951299325 and b is 90.36851181362712\n",
      "Iteration 323, the loss is 227957.13614332938, parameters k is -86.0111148786059 and b is 90.36951181362713\n",
      "Iteration 324, the loss is 227918.26232239878, parameters k is -86.00483024421854 and b is 90.37051181362713\n",
      "Iteration 325, the loss is 227879.39182034167, parameters k is -85.99854560983118 and b is 90.37151181362714\n",
      "Iteration 326, the loss is 227840.52463715826, parameters k is -85.99226097544383 and b is 90.37251181362714\n",
      "Iteration 327, the loss is 227801.6607728485, parameters k is -85.98597634105647 and b is 90.37351181362715\n",
      "Iteration 328, the loss is 227762.80022741246, parameters k is -85.97969170666912 and b is 90.37451181362715\n",
      "Iteration 329, the loss is 227723.94300084945, parameters k is -85.97340707228176 and b is 90.37551181362716\n",
      "Iteration 330, the loss is 227685.08909316038, parameters k is -85.9671224378944 and b is 90.37651181362716\n",
      "Iteration 331, the loss is 227646.23850434515, parameters k is -85.96083780350705 and b is 90.37751181362717\n",
      "Iteration 332, the loss is 227607.3912344035, parameters k is -85.95455316911969 and b is 90.37851181362717\n",
      "Iteration 333, the loss is 227568.54728333472, parameters k is -85.94826853473234 and b is 90.37951181362718\n",
      "Iteration 334, the loss is 227529.70665113986, parameters k is -85.94198390034498 and b is 90.38051181362718\n",
      "Iteration 335, the loss is 227490.8693378191, parameters k is -85.93569926595762 and b is 90.38151181362718\n",
      "Iteration 336, the loss is 227452.0353433716, parameters k is -85.92941463157027 and b is 90.38251181362719\n",
      "Iteration 337, the loss is 227413.20466779737, parameters k is -85.92312999718291 and b is 90.3835118136272\n",
      "Iteration 338, the loss is 227374.37731109693, parameters k is -85.91684536279556 and b is 90.3845118136272\n",
      "Iteration 339, the loss is 227335.55327327014, parameters k is -85.9105607284082 and b is 90.3855118136272\n",
      "Iteration 340, the loss is 227296.73255431696, parameters k is -85.90427609402084 and b is 90.38651181362721\n",
      "Iteration 341, the loss is 227257.91515423686, parameters k is -85.89799145963349 and b is 90.38751181362721\n",
      "Iteration 342, the loss is 227219.10107303128, parameters k is -85.89170682524613 and b is 90.38851181362722\n",
      "Iteration 343, the loss is 227180.29031069897, parameters k is -85.88542219085878 and b is 90.38951181362722\n",
      "Iteration 344, the loss is 227141.48286723977, parameters k is -85.87913755647142 and b is 90.39051181362723\n",
      "Iteration 345, the loss is 227102.6787426544, parameters k is -85.87285292208406 and b is 90.39151181362723\n",
      "Iteration 346, the loss is 227063.8779369426, parameters k is -85.86656828769671 and b is 90.39251181362724\n",
      "Iteration 347, the loss is 227025.0804501045, parameters k is -85.86028365330935 and b is 90.39351181362724\n",
      "Iteration 348, the loss is 226986.2862821401, parameters k is -85.853999018922 and b is 90.39451181362725\n",
      "Iteration 349, the loss is 226947.49543304904, parameters k is -85.84771438453464 and b is 90.39551181362725\n",
      "Iteration 350, the loss is 226908.7079028314, parameters k is -85.84142975014728 and b is 90.39651181362726\n",
      "Iteration 351, the loss is 226869.92369148778, parameters k is -85.83514511575993 and b is 90.39751181362726\n",
      "Iteration 352, the loss is 226831.1427990172, parameters k is -85.82886048137257 and b is 90.39851181362727\n",
      "Iteration 353, the loss is 226792.36522542083, parameters k is -85.82257584698522 and b is 90.39951181362727\n",
      "Iteration 354, the loss is 226753.5909706977, parameters k is -85.81629121259786 and b is 90.40051181362728\n",
      "Iteration 355, the loss is 226714.82003484803, parameters k is -85.8100065782105 and b is 90.40151181362728\n",
      "Iteration 356, the loss is 226676.05241787253, parameters k is -85.80372194382315 and b is 90.40251181362729\n",
      "Iteration 357, the loss is 226637.28811976992, parameters k is -85.7974373094358 and b is 90.40351181362729\n",
      "Iteration 358, the loss is 226598.52714054103, parameters k is -85.79115267504844 and b is 90.4045118136273\n",
      "Iteration 359, the loss is 226559.76948018596, parameters k is -85.78486804066108 and b is 90.4055118136273\n",
      "Iteration 360, the loss is 226521.0151387048, parameters k is -85.77858340627373 and b is 90.4065118136273\n",
      "Iteration 361, the loss is 226482.26411609666, parameters k is -85.77229877188637 and b is 90.40751181362731\n",
      "Iteration 362, the loss is 226443.51641236193, parameters k is -85.76601413749901 and b is 90.40851181362731\n",
      "Iteration 363, the loss is 226404.7720275012, parameters k is -85.75972950311166 and b is 90.40951181362732\n",
      "Iteration 364, the loss is 226366.03096151413, parameters k is -85.7534448687243 and b is 90.41051181362732\n",
      "Iteration 365, the loss is 226327.2932144005, parameters k is -85.74716023433695 and b is 90.41151181362733\n",
      "Iteration 366, the loss is 226288.55878616028, parameters k is -85.74087559994959 and b is 90.41251181362733\n",
      "Iteration 367, the loss is 226249.82767679388, parameters k is -85.73459096556223 and b is 90.41351181362734\n",
      "Iteration 368, the loss is 226211.09988630065, parameters k is -85.72830633117488 and b is 90.41451181362734\n",
      "Iteration 369, the loss is 226172.37541468145, parameters k is -85.72202169678752 and b is 90.41551181362735\n",
      "Iteration 370, the loss is 226133.6542619357, parameters k is -85.71573706240017 and b is 90.41651181362735\n",
      "Iteration 371, the loss is 226094.93642806355, parameters k is -85.70945242801281 and b is 90.41751181362736\n",
      "Iteration 372, the loss is 226056.22191306518, parameters k is -85.70316779362545 and b is 90.41851181362736\n",
      "Iteration 373, the loss is 226017.5107169399, parameters k is -85.6968831592381 and b is 90.41951181362737\n",
      "Iteration 374, the loss is 225978.80283968878, parameters k is -85.69059852485074 and b is 90.42051181362737\n",
      "Iteration 375, the loss is 225940.09828131073, parameters k is -85.68431389046339 and b is 90.42151181362738\n",
      "Iteration 376, the loss is 225901.39704180657, parameters k is -85.67802925607603 and b is 90.42251181362738\n",
      "Iteration 377, the loss is 225862.6991211759, parameters k is -85.67174462168867 and b is 90.42351181362739\n",
      "Iteration 378, the loss is 225824.00451941916, parameters k is -85.66545998730132 and b is 90.42451181362739\n",
      "Iteration 379, the loss is 225785.3132365352, parameters k is -85.65917535291396 and b is 90.4255118136274\n",
      "Iteration 380, the loss is 225746.62527252553, parameters k is -85.6528907185266 and b is 90.4265118136274\n",
      "Iteration 381, the loss is 225707.94062738935, parameters k is -85.64660608413925 and b is 90.4275118136274\n",
      "Iteration 382, the loss is 225669.2593011263, parameters k is -85.6403214497519 and b is 90.42851181362741\n",
      "Iteration 383, the loss is 225630.5812937374, parameters k is -85.63403681536454 and b is 90.42951181362741\n",
      "Iteration 384, the loss is 225591.90660522174, parameters k is -85.62775218097718 and b is 90.43051181362742\n",
      "Iteration 385, the loss is 225553.2352355798, parameters k is -85.62146754658983 and b is 90.43151181362742\n",
      "Iteration 386, the loss is 225514.5671848113, parameters k is -85.61518291220247 and b is 90.43251181362743\n",
      "Iteration 387, the loss is 225475.90245291617, parameters k is -85.60889827781511 and b is 90.43351181362743\n",
      "Iteration 388, the loss is 225437.24103989542, parameters k is -85.60261364342776 and b is 90.43451181362744\n",
      "Iteration 389, the loss is 225398.58294574733, parameters k is -85.5963290090404 and b is 90.43551181362744\n",
      "Iteration 390, the loss is 225359.92817047331, parameters k is -85.59004437465305 and b is 90.43651181362745\n",
      "Iteration 391, the loss is 225321.27671407326, parameters k is -85.58375974026569 and b is 90.43751181362745\n",
      "Iteration 392, the loss is 225282.6285765461, parameters k is -85.57747510587834 and b is 90.43851181362746\n",
      "Iteration 393, the loss is 225243.983757893, parameters k is -85.57119047149098 and b is 90.43951181362746\n",
      "Iteration 394, the loss is 225205.34225811297, parameters k is -85.56490583710362 and b is 90.44051181362747\n",
      "Iteration 395, the loss is 225166.704077207, parameters k is -85.55862120271627 and b is 90.44151181362747\n",
      "Iteration 396, the loss is 225128.06921517442, parameters k is -85.55233656832891 and b is 90.44251181362748\n",
      "Iteration 397, the loss is 225089.43767201525, parameters k is -85.54605193394156 and b is 90.44351181362748\n",
      "Iteration 398, the loss is 225050.80944772987, parameters k is -85.5397672995542 and b is 90.44451181362749\n",
      "Iteration 399, the loss is 225012.18454231814, parameters k is -85.53348266516684 and b is 90.44551181362749\n",
      "Iteration 400, the loss is 224973.56295577966, parameters k is -85.52719803077949 and b is 90.4465118136275\n",
      "Iteration 401, the loss is 224934.9446881155, parameters k is -85.52091339639213 and b is 90.4475118136275\n",
      "Iteration 402, the loss is 224896.32973932414, parameters k is -85.51462876200478 and b is 90.4485118136275\n",
      "Iteration 403, the loss is 224857.71810940665, parameters k is -85.50834412761742 and b is 90.44951181362751\n",
      "Iteration 404, the loss is 224819.1097983628, parameters k is -85.50205949323006 and b is 90.45051181362751\n",
      "Iteration 405, the loss is 224780.50480619248, parameters k is -85.49577485884271 and b is 90.45151181362752\n",
      "Iteration 406, the loss is 224741.90313289582, parameters k is -85.48949022445535 and b is 90.45251181362752\n",
      "Iteration 407, the loss is 224703.3047784725, parameters k is -85.483205590068 and b is 90.45351181362753\n",
      "Iteration 408, the loss is 224664.709742923, parameters k is -85.47692095568064 and b is 90.45451181362753\n",
      "Iteration 409, the loss is 224626.1180262471, parameters k is -85.47063632129328 and b is 90.45551181362754\n",
      "Iteration 410, the loss is 224587.52962844435, parameters k is -85.46435168690593 and b is 90.45651181362754\n",
      "Iteration 411, the loss is 224548.94454951596, parameters k is -85.45806705251857 and b is 90.45751181362755\n",
      "Iteration 412, the loss is 224510.36278946046, parameters k is -85.45178241813122 and b is 90.45851181362755\n",
      "Iteration 413, the loss is 224471.7843482789, parameters k is -85.44549778374386 and b is 90.45951181362756\n",
      "Iteration 414, the loss is 224433.20922597076, parameters k is -85.4392131493565 and b is 90.46051181362756\n",
      "Iteration 415, the loss is 224394.63742253612, parameters k is -85.43292851496915 and b is 90.46151181362757\n",
      "Iteration 416, the loss is 224356.0689379755, parameters k is -85.42664388058179 and b is 90.46251181362757\n",
      "Iteration 417, the loss is 224317.503772288, parameters k is -85.42035924619444 and b is 90.46351181362758\n",
      "Iteration 418, the loss is 224278.9419254744, parameters k is -85.41407461180708 and b is 90.46451181362758\n",
      "Iteration 419, the loss is 224240.3833975344, parameters k is -85.40778997741972 and b is 90.46551181362759\n",
      "Iteration 420, the loss is 224201.82818846765, parameters k is -85.40150534303237 and b is 90.46651181362759\n",
      "Iteration 421, the loss is 224163.27629827472, parameters k is -85.39522070864501 and b is 90.4675118136276\n",
      "Iteration 422, the loss is 224124.72772695532, parameters k is -85.38893607425766 and b is 90.4685118136276\n",
      "Iteration 423, the loss is 224086.1824745093, parameters k is -85.3826514398703 and b is 90.4695118136276\n",
      "Iteration 424, the loss is 224047.6405409374, parameters k is -85.37636680548295 and b is 90.47051181362761\n",
      "Iteration 425, the loss is 224009.10192623865, parameters k is -85.37008217109559 and b is 90.47151181362761\n",
      "Iteration 426, the loss is 223970.56663041364, parameters k is -85.36379753670823 and b is 90.47251181362762\n",
      "Iteration 427, the loss is 223932.03465346177, parameters k is -85.35751290232088 and b is 90.47351181362762\n",
      "Iteration 428, the loss is 223893.50599538398, parameters k is -85.35122826793352 and b is 90.47451181362763\n",
      "Iteration 429, the loss is 223854.98065617974, parameters k is -85.34494363354617 and b is 90.47551181362763\n",
      "Iteration 430, the loss is 223816.45863584892, parameters k is -85.33865899915881 and b is 90.47651181362764\n",
      "Iteration 431, the loss is 223777.93993439182, parameters k is -85.33237436477145 and b is 90.47751181362764\n",
      "Iteration 432, the loss is 223739.42455180842, parameters k is -85.3260897303841 and b is 90.47851181362765\n",
      "Iteration 433, the loss is 223700.91248809837, parameters k is -85.31980509599674 and b is 90.47951181362765\n",
      "Iteration 434, the loss is 223662.40374326202, parameters k is -85.31352046160939 and b is 90.48051181362766\n",
      "Iteration 435, the loss is 223623.89831729938, parameters k is -85.30723582722203 and b is 90.48151181362766\n",
      "Iteration 436, the loss is 223585.39621021002, parameters k is -85.30095119283467 and b is 90.48251181362767\n",
      "Iteration 437, the loss is 223546.89742199425, parameters k is -85.29466655844732 and b is 90.48351181362767\n",
      "Iteration 438, the loss is 223508.4019526522, parameters k is -85.28838192405996 and b is 90.48451181362768\n",
      "Iteration 439, the loss is 223469.90980218374, parameters k is -85.2820972896726 and b is 90.48551181362768\n",
      "Iteration 440, the loss is 223431.42097058884, parameters k is -85.27581265528525 and b is 90.48651181362769\n",
      "Iteration 441, the loss is 223392.93545786728, parameters k is -85.2695280208979 and b is 90.48751181362769\n",
      "Iteration 442, the loss is 223354.45326401992, parameters k is -85.26324338651054 and b is 90.4885118136277\n",
      "Iteration 443, the loss is 223315.97438904567, parameters k is -85.25695875212318 and b is 90.4895118136277\n",
      "Iteration 444, the loss is 223277.49883294493, parameters k is -85.25067411773583 and b is 90.4905118136277\n",
      "Iteration 445, the loss is 223239.02659571802, parameters k is -85.24438948334847 and b is 90.49151181362771\n",
      "Iteration 446, the loss is 223200.5576773645, parameters k is -85.23810484896111 and b is 90.49251181362771\n",
      "Iteration 447, the loss is 223162.0920778848, parameters k is -85.23182021457376 and b is 90.49351181362772\n",
      "Iteration 448, the loss is 223123.62979727838, parameters k is -85.2255355801864 and b is 90.49451181362772\n",
      "Iteration 449, the loss is 223085.1708355457, parameters k is -85.21925094579905 and b is 90.49551181362773\n",
      "Iteration 450, the loss is 223046.71519268706, parameters k is -85.21296631141169 and b is 90.49651181362773\n",
      "Iteration 451, the loss is 223008.26286870157, parameters k is -85.20668167702433 and b is 90.49751181362774\n",
      "Iteration 452, the loss is 222969.81386358946, parameters k is -85.20039704263698 and b is 90.49851181362774\n",
      "Iteration 453, the loss is 222931.36817735084, parameters k is -85.19411240824962 and b is 90.49951181362775\n",
      "Iteration 454, the loss is 222892.92580998637, parameters k is -85.18782777386227 and b is 90.50051181362775\n",
      "Iteration 455, the loss is 222854.48676149518, parameters k is -85.18154313947491 and b is 90.50151181362776\n",
      "Iteration 456, the loss is 222816.0510318778, parameters k is -85.17525850508756 and b is 90.50251181362776\n",
      "Iteration 457, the loss is 222777.61862113373, parameters k is -85.1689738707002 and b is 90.50351181362777\n",
      "Iteration 458, the loss is 222739.1895292633, parameters k is -85.16268923631284 and b is 90.50451181362777\n",
      "Iteration 459, the loss is 222700.76375626662, parameters k is -85.15640460192549 and b is 90.50551181362778\n",
      "Iteration 460, the loss is 222662.34130214312, parameters k is -85.15011996753813 and b is 90.50651181362778\n",
      "Iteration 461, the loss is 222623.92216689387, parameters k is -85.14383533315078 and b is 90.50751181362779\n",
      "Iteration 462, the loss is 222585.50635051727, parameters k is -85.13755069876342 and b is 90.50851181362779\n",
      "Iteration 463, the loss is 222547.09385301525, parameters k is -85.13126606437606 and b is 90.5095118136278\n",
      "Iteration 464, the loss is 222508.68467438626, parameters k is -85.12498142998871 and b is 90.5105118136278\n",
      "Iteration 465, the loss is 222470.2788146308, parameters k is -85.11869679560135 and b is 90.5115118136278\n",
      "Iteration 466, the loss is 222431.87627374902, parameters k is -85.112412161214 and b is 90.51251181362781\n",
      "Iteration 467, the loss is 222393.47705174098, parameters k is -85.10612752682664 and b is 90.51351181362782\n",
      "Iteration 468, the loss is 222355.08114860646, parameters k is -85.09984289243928 and b is 90.51451181362782\n",
      "Iteration 469, the loss is 222316.68856434565, parameters k is -85.09355825805193 and b is 90.51551181362782\n",
      "Iteration 470, the loss is 222278.2992989581, parameters k is -85.08727362366457 and b is 90.51651181362783\n",
      "Iteration 471, the loss is 222239.91335244424, parameters k is -85.08098898927722 and b is 90.51751181362783\n",
      "Iteration 472, the loss is 222201.53072480424, parameters k is -85.07470435488986 and b is 90.51851181362784\n",
      "Iteration 473, the loss is 222163.15141603717, parameters k is -85.0684197205025 and b is 90.51951181362784\n",
      "Iteration 474, the loss is 222124.77542614433, parameters k is -85.06213508611515 and b is 90.52051181362785\n",
      "Iteration 475, the loss is 222086.40275512484, parameters k is -85.05585045172779 and b is 90.52151181362785\n",
      "Iteration 476, the loss is 222048.0334029789, parameters k is -85.04956581734044 and b is 90.52251181362786\n",
      "Iteration 477, the loss is 222009.66736970644, parameters k is -85.04328118295308 and b is 90.52351181362786\n",
      "Iteration 478, the loss is 221971.30465530782, parameters k is -85.03699654856572 and b is 90.52451181362787\n",
      "Iteration 479, the loss is 221932.9452597827, parameters k is -85.03071191417837 and b is 90.52551181362787\n",
      "Iteration 480, the loss is 221894.5891831315, parameters k is -85.02442727979101 and b is 90.52651181362788\n",
      "Iteration 481, the loss is 221856.236425353, parameters k is -85.01814264540366 and b is 90.52751181362788\n",
      "Iteration 482, the loss is 221817.88698644837, parameters k is -85.0118580110163 and b is 90.52851181362789\n",
      "Iteration 483, the loss is 221779.540866418, parameters k is -85.00557337662894 and b is 90.52951181362789\n",
      "Iteration 484, the loss is 221741.19806526072, parameters k is -84.99928874224159 and b is 90.5305118136279\n",
      "Iteration 485, the loss is 221702.85858297712, parameters k is -84.99300410785423 and b is 90.5315118136279\n",
      "Iteration 486, the loss is 221664.52241956693, parameters k is -84.98671947346688 and b is 90.5325118136279\n",
      "Iteration 487, the loss is 221626.18957503053, parameters k is -84.98043483907952 and b is 90.53351181362791\n",
      "Iteration 488, the loss is 221587.86004936753, parameters k is -84.97415020469217 and b is 90.53451181362792\n",
      "Iteration 489, the loss is 221549.53384257844, parameters k is -84.96786557030481 and b is 90.53551181362792\n",
      "Iteration 490, the loss is 221511.21095466265, parameters k is -84.96158093591745 and b is 90.53651181362792\n",
      "Iteration 491, the loss is 221472.89138562066, parameters k is -84.9552963015301 and b is 90.53751181362793\n",
      "Iteration 492, the loss is 221434.5751354519, parameters k is -84.94901166714274 and b is 90.53851181362793\n",
      "Iteration 493, the loss is 221396.262204157, parameters k is -84.94272703275539 and b is 90.53951181362794\n",
      "Iteration 494, the loss is 221357.95259173558, parameters k is -84.93644239836803 and b is 90.54051181362794\n",
      "Iteration 495, the loss is 221319.64629818784, parameters k is -84.93015776398067 and b is 90.54151181362795\n",
      "Iteration 496, the loss is 221281.34332351323, parameters k is -84.92387312959332 and b is 90.54251181362795\n",
      "Iteration 497, the loss is 221243.04366771286, parameters k is -84.91758849520596 and b is 90.54351181362796\n",
      "Iteration 498, the loss is 221204.7473307858, parameters k is -84.9113038608186 and b is 90.54451181362796\n",
      "Iteration 499, the loss is 221166.45431273212, parameters k is -84.90501922643125 and b is 90.54551181362797\n",
      "Iteration 500, the loss is 221128.16461355245, parameters k is -84.8987345920439 and b is 90.54651181362797\n",
      "Iteration 501, the loss is 221089.87823324604, parameters k is -84.89244995765654 and b is 90.54751181362798\n",
      "Iteration 502, the loss is 221051.59517181336, parameters k is -84.88616532326918 and b is 90.54851181362798\n",
      "Iteration 503, the loss is 221013.31542925397, parameters k is -84.87988068888183 and b is 90.54951181362799\n",
      "Iteration 504, the loss is 220975.03900556863, parameters k is -84.87359605449447 and b is 90.55051181362799\n",
      "Iteration 505, the loss is 220936.76590075658, parameters k is -84.86731142010711 and b is 90.551511813628\n",
      "Iteration 506, the loss is 220898.4961148182, parameters k is -84.86102678571976 and b is 90.552511813628\n",
      "Iteration 507, the loss is 220860.22964775347, parameters k is -84.8547421513324 and b is 90.553511813628\n",
      "Iteration 508, the loss is 220821.9664995622, parameters k is -84.84845751694505 and b is 90.55451181362801\n",
      "Iteration 509, the loss is 220783.70667024457, parameters k is -84.84217288255769 and b is 90.55551181362802\n",
      "Iteration 510, the loss is 220745.45015980047, parameters k is -84.83588824817033 and b is 90.55651181362802\n",
      "Iteration 511, the loss is 220707.19696822995, parameters k is -84.82960361378298 and b is 90.55751181362803\n",
      "Iteration 512, the loss is 220668.94709553313, parameters k is -84.82331897939562 and b is 90.55851181362803\n",
      "Iteration 513, the loss is 220630.7005417098, parameters k is -84.81703434500827 and b is 90.55951181362803\n",
      "Iteration 514, the loss is 220592.45730676013, parameters k is -84.81074971062091 and b is 90.56051181362804\n",
      "Iteration 515, the loss is 220554.21739068386, parameters k is -84.80446507623356 and b is 90.56151181362804\n",
      "Iteration 516, the loss is 220515.98079348137, parameters k is -84.7981804418462 and b is 90.56251181362805\n",
      "Iteration 517, the loss is 220477.74751515227, parameters k is -84.79189580745884 and b is 90.56351181362805\n",
      "Iteration 518, the loss is 220439.51755569677, parameters k is -84.78561117307149 and b is 90.56451181362806\n",
      "Iteration 519, the loss is 220401.29091511515, parameters k is -84.77932653868413 and b is 90.56551181362806\n",
      "Iteration 520, the loss is 220363.06759340674, parameters k is -84.77304190429678 and b is 90.56651181362807\n",
      "Iteration 521, the loss is 220324.8475905726, parameters k is -84.76675726990942 and b is 90.56751181362807\n",
      "Iteration 522, the loss is 220286.63090661116, parameters k is -84.76047263552206 and b is 90.56851181362808\n",
      "Iteration 523, the loss is 220248.41754152393, parameters k is -84.75418800113471 and b is 90.56951181362808\n",
      "Iteration 524, the loss is 220210.2074953101, parameters k is -84.74790336674735 and b is 90.57051181362809\n",
      "Iteration 525, the loss is 220172.0007679697, parameters k is -84.74161873236 and b is 90.57151181362809\n",
      "Iteration 526, the loss is 220133.7973595027, parameters k is -84.73533409797264 and b is 90.5725118136281\n",
      "Iteration 527, the loss is 220095.59726990972, parameters k is -84.72904946358528 and b is 90.5735118136281\n",
      "Iteration 528, the loss is 220057.40049919003, parameters k is -84.72276482919793 and b is 90.5745118136281\n",
      "Iteration 529, the loss is 220019.20704734427, parameters k is -84.71648019481057 and b is 90.57551181362811\n",
      "Iteration 530, the loss is 219981.01691437163, parameters k is -84.71019556042322 and b is 90.57651181362812\n",
      "Iteration 531, the loss is 219942.83010027304, parameters k is -84.70391092603586 and b is 90.57751181362812\n",
      "Iteration 532, the loss is 219904.6466050478, parameters k is -84.6976262916485 and b is 90.57851181362813\n",
      "Iteration 533, the loss is 219866.46642869583, parameters k is -84.69134165726115 and b is 90.57951181362813\n",
      "Iteration 534, the loss is 219828.28957121805, parameters k is -84.68505702287379 and b is 90.58051181362814\n",
      "Iteration 535, the loss is 219790.11603261324, parameters k is -84.67877238848644 and b is 90.58151181362814\n",
      "Iteration 536, the loss is 219751.9458128827, parameters k is -84.67248775409908 and b is 90.58251181362814\n",
      "Iteration 537, the loss is 219713.7789120252, parameters k is -84.66620311971172 and b is 90.58351181362815\n",
      "Iteration 538, the loss is 219675.61533004168, parameters k is -84.65991848532437 and b is 90.58451181362815\n",
      "Iteration 539, the loss is 219637.45506693143, parameters k is -84.65363385093701 and b is 90.58551181362816\n",
      "Iteration 540, the loss is 219599.29812269492, parameters k is -84.64734921654966 and b is 90.58651181362816\n",
      "Iteration 541, the loss is 219561.14449733196, parameters k is -84.6410645821623 and b is 90.58751181362817\n",
      "Iteration 542, the loss is 219522.99419084244, parameters k is -84.63477994777494 and b is 90.58851181362817\n",
      "Iteration 543, the loss is 219484.84720322682, parameters k is -84.62849531338759 and b is 90.58951181362818\n",
      "Iteration 544, the loss is 219446.70353448475, parameters k is -84.62221067900023 and b is 90.59051181362818\n",
      "Iteration 545, the loss is 219408.5631846156, parameters k is -84.61592604461288 and b is 90.59151181362819\n",
      "Iteration 546, the loss is 219370.4261536208, parameters k is -84.60964141022552 and b is 90.59251181362819\n",
      "Iteration 547, the loss is 219332.29244149945, parameters k is -84.60335677583817 and b is 90.5935118136282\n",
      "Iteration 548, the loss is 219294.1620482514, parameters k is -84.59707214145081 and b is 90.5945118136282\n",
      "Iteration 549, the loss is 219256.03497387734, parameters k is -84.59078750706345 and b is 90.5955118136282\n",
      "Iteration 550, the loss is 219217.91121837642, parameters k is -84.5845028726761 and b is 90.59651181362821\n",
      "Iteration 551, the loss is 219179.79078174915, parameters k is -84.57821823828874 and b is 90.59751181362822\n",
      "Iteration 552, the loss is 219141.67366399584, parameters k is -84.57193360390139 and b is 90.59851181362822\n",
      "Iteration 553, the loss is 219103.559865116, parameters k is -84.56564896951403 and b is 90.59951181362823\n",
      "Iteration 554, the loss is 219065.44938510933, parameters k is -84.55936433512667 and b is 90.60051181362823\n",
      "Iteration 555, the loss is 219027.34222397665, parameters k is -84.55307970073932 and b is 90.60151181362824\n",
      "Iteration 556, the loss is 218989.23838171765, parameters k is -84.54679506635196 and b is 90.60251181362824\n",
      "Iteration 557, the loss is 218951.1378583318, parameters k is -84.5405104319646 and b is 90.60351181362824\n",
      "Iteration 558, the loss is 218913.04065381965, parameters k is -84.53422579757725 and b is 90.60451181362825\n",
      "Iteration 559, the loss is 218874.9467681813, parameters k is -84.5279411631899 and b is 90.60551181362825\n",
      "Iteration 560, the loss is 218836.8562014162, parameters k is -84.52165652880254 and b is 90.60651181362826\n",
      "Iteration 561, the loss is 218798.7689535253, parameters k is -84.51537189441518 and b is 90.60751181362826\n",
      "Iteration 562, the loss is 218760.68502450731, parameters k is -84.50908726002783 and b is 90.60851181362827\n",
      "Iteration 563, the loss is 218722.6044143631, parameters k is -84.50280262564047 and b is 90.60951181362827\n",
      "Iteration 564, the loss is 218684.52712309256, parameters k is -84.49651799125311 and b is 90.61051181362828\n",
      "Iteration 565, the loss is 218646.45315069545, parameters k is -84.49023335686576 and b is 90.61151181362828\n",
      "Iteration 566, the loss is 218608.3824971722, parameters k is -84.4839487224784 and b is 90.61251181362829\n",
      "Iteration 567, the loss is 218570.3151625223, parameters k is -84.47766408809105 and b is 90.61351181362829\n",
      "Iteration 568, the loss is 218532.25114674625, parameters k is -84.47137945370369 and b is 90.6145118136283\n",
      "Iteration 569, the loss is 218494.1904498434, parameters k is -84.46509481931633 and b is 90.6155118136283\n",
      "Iteration 570, the loss is 218456.1330718143, parameters k is -84.45881018492898 and b is 90.6165118136283\n",
      "Iteration 571, the loss is 218418.07901265906, parameters k is -84.45252555054162 and b is 90.61751181362831\n",
      "Iteration 572, the loss is 218380.028272377, parameters k is -84.44624091615427 and b is 90.61851181362832\n",
      "Iteration 573, the loss is 218341.98085096886, parameters k is -84.43995628176691 and b is 90.61951181362832\n",
      "Iteration 574, the loss is 218303.93674843395, parameters k is -84.43367164737955 and b is 90.62051181362833\n",
      "Iteration 575, the loss is 218265.8959647731, parameters k is -84.4273870129922 and b is 90.62151181362833\n",
      "Iteration 576, the loss is 218227.85849998525, parameters k is -84.42110237860484 and b is 90.62251181362834\n",
      "Iteration 577, the loss is 218189.82435407152, parameters k is -84.41481774421749 and b is 90.62351181362834\n",
      "Iteration 578, the loss is 218151.79352703085, parameters k is -84.40853310983013 and b is 90.62451181362835\n",
      "Iteration 579, the loss is 218113.76601886417, parameters k is -84.40224847544278 and b is 90.62551181362835\n",
      "Iteration 580, the loss is 218075.74182957105, parameters k is -84.39596384105542 and b is 90.62651181362835\n",
      "Iteration 581, the loss is 218037.72095915143, parameters k is -84.38967920666806 and b is 90.62751181362836\n",
      "Iteration 582, the loss is 217999.70340760538, parameters k is -84.38339457228071 and b is 90.62851181362836\n",
      "Iteration 583, the loss is 217961.68917493286, parameters k is -84.37710993789335 and b is 90.62951181362837\n",
      "Iteration 584, the loss is 217923.67826113378, parameters k is -84.370825303506 and b is 90.63051181362837\n",
      "Iteration 585, the loss is 217885.6706662088, parameters k is -84.36454066911864 and b is 90.63151181362838\n",
      "Iteration 586, the loss is 217847.66639015696, parameters k is -84.35825603473128 and b is 90.63251181362838\n",
      "Iteration 587, the loss is 217809.6654329789, parameters k is -84.35197140034393 and b is 90.63351181362839\n",
      "Iteration 588, the loss is 217771.66779467431, parameters k is -84.34568676595657 and b is 90.6345118136284\n",
      "Iteration 589, the loss is 217733.67347524347, parameters k is -84.33940213156922 and b is 90.6355118136284\n",
      "Iteration 590, the loss is 217695.6824746858, parameters k is -84.33311749718186 and b is 90.6365118136284\n",
      "Iteration 591, the loss is 217657.6947930019, parameters k is -84.3268328627945 and b is 90.63751181362841\n",
      "Iteration 592, the loss is 217619.71043019206, parameters k is -84.32054822840715 and b is 90.63851181362841\n",
      "Iteration 593, the loss is 217581.7293862552, parameters k is -84.31426359401979 and b is 90.63951181362842\n",
      "Iteration 594, the loss is 217543.7516611921, parameters k is -84.30797895963244 and b is 90.64051181362842\n",
      "Iteration 595, the loss is 217505.77725500287, parameters k is -84.30169432524508 and b is 90.64151181362843\n",
      "Iteration 596, the loss is 217467.80616768677, parameters k is -84.29540969085772 and b is 90.64251181362843\n",
      "Iteration 597, the loss is 217429.83839924444, parameters k is -84.28912505647037 and b is 90.64351181362844\n",
      "Iteration 598, the loss is 217391.87394967602, parameters k is -84.28284042208301 and b is 90.64451181362844\n",
      "Iteration 599, the loss is 217353.9128189806, parameters k is -84.27655578769566 and b is 90.64551181362845\n",
      "Iteration 600, the loss is 217315.95500715915, parameters k is -84.2702711533083 and b is 90.64651181362845\n",
      "Iteration 601, the loss is 217278.00051421084, parameters k is -84.26398651892094 and b is 90.64751181362846\n",
      "Iteration 602, the loss is 217240.04934013664, parameters k is -84.25770188453359 and b is 90.64851181362846\n",
      "Iteration 603, the loss is 217202.10148493593, parameters k is -84.25141725014623 and b is 90.64951181362846\n",
      "Iteration 604, the loss is 217164.15694860893, parameters k is -84.24513261575888 and b is 90.65051181362847\n",
      "Iteration 605, the loss is 217126.21573115495, parameters k is -84.23884798137152 and b is 90.65151181362847\n",
      "Iteration 606, the loss is 217088.27783257485, parameters k is -84.23256334698416 and b is 90.65251181362848\n",
      "Iteration 607, the loss is 217050.34325286857, parameters k is -84.22627871259681 and b is 90.65351181362848\n",
      "Iteration 608, the loss is 217012.4119920356, parameters k is -84.21999407820945 and b is 90.65451181362849\n",
      "Iteration 609, the loss is 216974.4840500765, parameters k is -84.2137094438221 and b is 90.6555118136285\n",
      "Iteration 610, the loss is 216936.55942699054, parameters k is -84.20742480943474 and b is 90.6565118136285\n",
      "Iteration 611, the loss is 216898.6381227787, parameters k is -84.20114017504739 and b is 90.6575118136285\n",
      "Iteration 612, the loss is 216860.72013743978, parameters k is -84.19485554066003 and b is 90.65851181362851\n",
      "Iteration 613, the loss is 216822.8054709748, parameters k is -84.18857090627267 and b is 90.65951181362851\n",
      "Iteration 614, the loss is 216784.8941233836, parameters k is -84.18228627188532 and b is 90.66051181362852\n",
      "Iteration 615, the loss is 216746.98609466592, parameters k is -84.17600163749796 and b is 90.66151181362852\n",
      "Iteration 616, the loss is 216709.0813848218, parameters k is -84.1697170031106 and b is 90.66251181362853\n",
      "Iteration 617, the loss is 216671.17999385102, parameters k is -84.16343236872325 and b is 90.66351181362853\n",
      "Iteration 618, the loss is 216633.28192175398, parameters k is -84.1571477343359 and b is 90.66451181362854\n",
      "Iteration 619, the loss is 216595.38716853058, parameters k is -84.15086309994854 and b is 90.66551181362854\n",
      "Iteration 620, the loss is 216557.49573418067, parameters k is -84.14457846556118 and b is 90.66651181362855\n",
      "Iteration 621, the loss is 216519.60761870403, parameters k is -84.13829383117383 and b is 90.66751181362855\n",
      "Iteration 622, the loss is 216481.72282210147, parameters k is -84.13200919678647 and b is 90.66851181362856\n",
      "Iteration 623, the loss is 216443.84134437234, parameters k is -84.12572456239911 and b is 90.66951181362856\n",
      "Iteration 624, the loss is 216405.96318551697, parameters k is -84.11943992801176 and b is 90.67051181362856\n",
      "Iteration 625, the loss is 216368.08834553498, parameters k is -84.1131552936244 and b is 90.67151181362857\n",
      "Iteration 626, the loss is 216330.21682442646, parameters k is -84.10687065923705 and b is 90.67251181362857\n",
      "Iteration 627, the loss is 216292.34862219144, parameters k is -84.10058602484969 and b is 90.67351181362858\n",
      "Iteration 628, the loss is 216254.48373883046, parameters k is -84.09430139046233 and b is 90.67451181362858\n",
      "Iteration 629, the loss is 216216.62217434277, parameters k is -84.08801675607498 and b is 90.67551181362859\n",
      "Iteration 630, the loss is 216178.7639287289, parameters k is -84.08173212168762 and b is 90.6765118136286\n",
      "Iteration 631, the loss is 216140.9090019882, parameters k is -84.07544748730027 and b is 90.6775118136286\n",
      "Iteration 632, the loss is 216103.05739412163, parameters k is -84.06916285291291 and b is 90.6785118136286\n",
      "Iteration 633, the loss is 216065.2091051282, parameters k is -84.06287821852555 and b is 90.67951181362861\n",
      "Iteration 634, the loss is 216027.36413500799, parameters k is -84.0565935841382 and b is 90.68051181362861\n",
      "Iteration 635, the loss is 215989.52248376238, parameters k is -84.05030894975084 and b is 90.68151181362862\n",
      "Iteration 636, the loss is 215951.68415138967, parameters k is -84.04402431536349 and b is 90.68251181362862\n",
      "Iteration 637, the loss is 215913.84913789073, parameters k is -84.03773968097613 and b is 90.68351181362863\n",
      "Iteration 638, the loss is 215876.0174432652, parameters k is -84.03145504658877 and b is 90.68451181362863\n",
      "Iteration 639, the loss is 215838.18906751354, parameters k is -84.02517041220142 and b is 90.68551181362864\n",
      "Iteration 640, the loss is 215800.3640106353, parameters k is -84.01888577781406 and b is 90.68651181362864\n",
      "Iteration 641, the loss is 215762.54227263064, parameters k is -84.0126011434267 and b is 90.68751181362865\n",
      "Iteration 642, the loss is 215724.72385349972, parameters k is -84.00631650903935 and b is 90.68851181362865\n",
      "Iteration 643, the loss is 215686.90875324205, parameters k is -84.000031874652 and b is 90.68951181362866\n",
      "Iteration 644, the loss is 215649.09697185789, parameters k is -83.99374724026464 and b is 90.69051181362866\n",
      "Iteration 645, the loss is 215611.28850934806, parameters k is -83.98746260587728 and b is 90.69151181362867\n",
      "Iteration 646, the loss is 215573.48336571132, parameters k is -83.98117797148993 and b is 90.69251181362867\n",
      "Iteration 647, the loss is 215535.68154094805, parameters k is -83.97489333710257 and b is 90.69351181362867\n",
      "Iteration 648, the loss is 215497.88303505862, parameters k is -83.96860870271522 and b is 90.69451181362868\n",
      "Iteration 649, the loss is 215460.08784804243, parameters k is -83.96232406832786 and b is 90.69551181362868\n",
      "Iteration 650, the loss is 215422.29597990005, parameters k is -83.9560394339405 and b is 90.69651181362869\n",
      "Iteration 651, the loss is 215384.50743063173, parameters k is -83.94975479955315 and b is 90.6975118136287\n",
      "Iteration 652, the loss is 215346.72220023643, parameters k is -83.94347016516579 and b is 90.6985118136287\n",
      "Iteration 653, the loss is 215308.94028871454, parameters k is -83.93718553077844 and b is 90.6995118136287\n",
      "Iteration 654, the loss is 215271.16169606647, parameters k is -83.93090089639108 and b is 90.70051181362871\n",
      "Iteration 655, the loss is 215233.3864222922, parameters k is -83.92461626200372 and b is 90.70151181362871\n",
      "Iteration 656, the loss is 215195.61446739116, parameters k is -83.91833162761637 and b is 90.70251181362872\n",
      "Iteration 657, the loss is 215157.84583136401, parameters k is -83.91204699322901 and b is 90.70351181362872\n",
      "Iteration 658, the loss is 215120.08051421033, parameters k is -83.90576235884166 and b is 90.70451181362873\n",
      "Iteration 659, the loss is 215082.31851592992, parameters k is -83.8994777244543 and b is 90.70551181362873\n",
      "Iteration 660, the loss is 215044.5598365232, parameters k is -83.89319309006694 and b is 90.70651181362874\n",
      "Iteration 661, the loss is 215006.80447599074, parameters k is -83.88690845567959 and b is 90.70751181362874\n",
      "Iteration 662, the loss is 214969.05243433084, parameters k is -83.88062382129223 and b is 90.70851181362875\n",
      "Iteration 663, the loss is 214931.30371154574, parameters k is -83.87433918690488 and b is 90.70951181362875\n",
      "Iteration 664, the loss is 214893.5583076329, parameters k is -83.86805455251752 and b is 90.71051181362876\n",
      "Iteration 665, the loss is 214855.8162225947, parameters k is -83.86176991813016 and b is 90.71151181362876\n",
      "Iteration 666, the loss is 214818.0774564295, parameters k is -83.85548528374281 and b is 90.71251181362877\n",
      "Iteration 667, the loss is 214780.34200913814, parameters k is -83.84920064935545 and b is 90.71351181362877\n",
      "Iteration 668, the loss is 214742.6098807203, parameters k is -83.8429160149681 and b is 90.71451181362877\n",
      "Iteration 669, the loss is 214704.88107117583, parameters k is -83.83663138058074 and b is 90.71551181362878\n",
      "Iteration 670, the loss is 214667.1555805053, parameters k is -83.83034674619338 and b is 90.71651181362878\n",
      "Iteration 671, the loss is 214629.4334087079, parameters k is -83.82406211180603 and b is 90.71751181362879\n",
      "Iteration 672, the loss is 214591.71455578462, parameters k is -83.81777747741867 and b is 90.7185118136288\n",
      "Iteration 673, the loss is 214553.99902173455, parameters k is -83.81149284303132 and b is 90.7195118136288\n",
      "Iteration 674, the loss is 214516.28680655817, parameters k is -83.80520820864396 and b is 90.7205118136288\n",
      "Iteration 675, the loss is 214478.57791025523, parameters k is -83.7989235742566 and b is 90.72151181362881\n",
      "Iteration 676, the loss is 214440.87233282602, parameters k is -83.79263893986925 and b is 90.72251181362881\n",
      "Iteration 677, the loss is 214403.17007427057, parameters k is -83.7863543054819 and b is 90.72351181362882\n",
      "Iteration 678, the loss is 214365.47113458833, parameters k is -83.78006967109454 and b is 90.72451181362882\n",
      "Iteration 679, the loss is 214327.77551378004, parameters k is -83.77378503670718 and b is 90.72551181362883\n",
      "Iteration 680, the loss is 214290.08321184522, parameters k is -83.76750040231983 and b is 90.72651181362883\n",
      "Iteration 681, the loss is 214252.39422878384, parameters k is -83.76121576793247 and b is 90.72751181362884\n",
      "Iteration 682, the loss is 214214.708564596, parameters k is -83.75493113354511 and b is 90.72851181362884\n",
      "Iteration 683, the loss is 214177.02621928204, parameters k is -83.74864649915776 and b is 90.72951181362885\n",
      "Iteration 684, the loss is 214139.34719284158, parameters k is -83.7423618647704 and b is 90.73051181362885\n",
      "Iteration 685, the loss is 214101.6714852743, parameters k is -83.73607723038305 and b is 90.73151181362886\n",
      "Iteration 686, the loss is 214063.99909658093, parameters k is -83.72979259599569 and b is 90.73251181362886\n",
      "Iteration 687, the loss is 214026.33002676113, parameters k is -83.72350796160833 and b is 90.73351181362887\n",
      "Iteration 688, the loss is 213988.66427581498, parameters k is -83.71722332722098 and b is 90.73451181362887\n",
      "Iteration 689, the loss is 213951.00184374256, parameters k is -83.71093869283362 and b is 90.73551181362888\n",
      "Iteration 690, the loss is 213913.3427305435, parameters k is -83.70465405844627 and b is 90.73651181362888\n",
      "Iteration 691, the loss is 213875.6869362179, parameters k is -83.69836942405891 and b is 90.73751181362888\n",
      "Iteration 692, the loss is 213838.0344607661, parameters k is -83.69208478967155 and b is 90.73851181362889\n",
      "Iteration 693, the loss is 213800.3853041876, parameters k is -83.6858001552842 and b is 90.7395118136289\n",
      "Iteration 694, the loss is 213762.73946648292, parameters k is -83.67951552089684 and b is 90.7405118136289\n",
      "Iteration 695, the loss is 213725.09694765188, parameters k is -83.67323088650949 and b is 90.7415118136289\n",
      "Iteration 696, the loss is 213687.45774769443, parameters k is -83.66694625212213 and b is 90.74251181362891\n",
      "Iteration 697, the loss is 213649.82186661035, parameters k is -83.66066161773477 and b is 90.74351181362891\n",
      "Iteration 698, the loss is 213612.18930440003, parameters k is -83.65437698334742 and b is 90.74451181362892\n",
      "Iteration 699, the loss is 213574.56006106298, parameters k is -83.64809234896006 and b is 90.74551181362892\n",
      "Iteration 700, the loss is 213536.93413660003, parameters k is -83.6418077145727 and b is 90.74651181362893\n",
      "Iteration 701, the loss is 213499.3115310104, parameters k is -83.63552308018535 and b is 90.74751181362893\n",
      "Iteration 702, the loss is 213461.6922442944, parameters k is -83.629238445798 and b is 90.74851181362894\n",
      "Iteration 703, the loss is 213424.07627645187, parameters k is -83.62295381141064 and b is 90.74951181362894\n",
      "Iteration 704, the loss is 213386.46362748314, parameters k is -83.61666917702328 and b is 90.75051181362895\n",
      "Iteration 705, the loss is 213348.85429738765, parameters k is -83.61038454263593 and b is 90.75151181362895\n",
      "Iteration 706, the loss is 213311.24828616588, parameters k is -83.60409990824857 and b is 90.75251181362896\n",
      "Iteration 707, the loss is 213273.6455938178, parameters k is -83.59781527386122 and b is 90.75351181362896\n",
      "Iteration 708, the loss is 213236.0462203435, parameters k is -83.59153063947386 and b is 90.75451181362897\n",
      "Iteration 709, the loss is 213198.45016574222, parameters k is -83.5852460050865 and b is 90.75551181362897\n",
      "Iteration 710, the loss is 213160.85743001514, parameters k is -83.57896137069915 and b is 90.75651181362898\n",
      "Iteration 711, the loss is 213123.26801316103, parameters k is -83.57267673631179 and b is 90.75751181362898\n",
      "Iteration 712, the loss is 213085.6819151809, parameters k is -83.56639210192444 and b is 90.75851181362899\n",
      "Iteration 713, the loss is 213048.09913607448, parameters k is -83.56010746753708 and b is 90.75951181362899\n",
      "Iteration 714, the loss is 213010.5196758411, parameters k is -83.55382283314972 and b is 90.760511813629\n",
      "Iteration 715, the loss is 212972.94353448178, parameters k is -83.54753819876237 and b is 90.761511813629\n",
      "Iteration 716, the loss is 212935.37071199613, parameters k is -83.54125356437501 and b is 90.762511813629\n",
      "Iteration 717, the loss is 212897.8012083836, parameters k is -83.53496892998766 and b is 90.76351181362901\n",
      "Iteration 718, the loss is 212860.2350236451, parameters k is -83.5286842956003 and b is 90.76451181362901\n",
      "Iteration 719, the loss is 212822.6721577798, parameters k is -83.52239966121294 and b is 90.76551181362902\n",
      "Iteration 720, the loss is 212785.112610788, parameters k is -83.51611502682559 and b is 90.76651181362902\n",
      "Iteration 721, the loss is 212747.5563826703, parameters k is -83.50983039243823 and b is 90.76751181362903\n",
      "Iteration 722, the loss is 212710.00347342604, parameters k is -83.50354575805088 and b is 90.76851181362903\n",
      "Iteration 723, the loss is 212672.45388305507, parameters k is -83.49726112366352 and b is 90.76951181362904\n",
      "Iteration 724, the loss is 212634.90761155766, parameters k is -83.49097648927616 and b is 90.77051181362904\n",
      "Iteration 725, the loss is 212597.3646589341, parameters k is -83.48469185488881 and b is 90.77151181362905\n",
      "Iteration 726, the loss is 212559.82502518428, parameters k is -83.47840722050145 and b is 90.77251181362905\n",
      "Iteration 727, the loss is 212522.288710308, parameters k is -83.4721225861141 and b is 90.77351181362906\n",
      "Iteration 728, the loss is 212484.7557143049, parameters k is -83.46583795172674 and b is 90.77451181362906\n",
      "Iteration 729, the loss is 212447.2260371754, parameters k is -83.45955331733938 and b is 90.77551181362907\n",
      "Iteration 730, the loss is 212409.69967891998, parameters k is -83.45326868295203 and b is 90.77651181362907\n",
      "Iteration 731, the loss is 212372.1766395379, parameters k is -83.44698404856467 and b is 90.77751181362908\n",
      "Iteration 732, the loss is 212334.65691902922, parameters k is -83.44069941417732 and b is 90.77851181362908\n",
      "Iteration 733, the loss is 212297.1405173942, parameters k is -83.43441477978996 and b is 90.77951181362909\n",
      "Iteration 734, the loss is 212259.6274346329, parameters k is -83.4281301454026 and b is 90.78051181362909\n",
      "Iteration 735, the loss is 212222.11767074504, parameters k is -83.42184551101525 and b is 90.7815118136291\n",
      "Iteration 736, the loss is 212184.61122573097, parameters k is -83.4155608766279 and b is 90.7825118136291\n",
      "Iteration 737, the loss is 212147.10809959043, parameters k is -83.40927624224054 and b is 90.7835118136291\n",
      "Iteration 738, the loss is 212109.60829232316, parameters k is -83.40299160785318 and b is 90.78451181362911\n",
      "Iteration 739, the loss is 212072.11180393008, parameters k is -83.39670697346583 and b is 90.78551181362911\n",
      "Iteration 740, the loss is 212034.61863440988, parameters k is -83.39042233907847 and b is 90.78651181362912\n",
      "Iteration 741, the loss is 211997.1287837636, parameters k is -83.38413770469111 and b is 90.78751181362912\n",
      "Iteration 742, the loss is 211959.6422519909, parameters k is -83.37785307030376 and b is 90.78851181362913\n",
      "Iteration 743, the loss is 211922.1590390916, parameters k is -83.3715684359164 and b is 90.78951181362913\n",
      "Iteration 744, the loss is 211884.6791450664, parameters k is -83.36528380152905 and b is 90.79051181362914\n",
      "Iteration 745, the loss is 211847.2025699143, parameters k is -83.35899916714169 and b is 90.79151181362914\n",
      "Iteration 746, the loss is 211809.729313636, parameters k is -83.35271453275433 and b is 90.79251181362915\n",
      "Iteration 747, the loss is 211772.25937623132, parameters k is -83.34642989836698 and b is 90.79351181362915\n",
      "Iteration 748, the loss is 211734.79275769953, parameters k is -83.34014526397962 and b is 90.79451181362916\n",
      "Iteration 749, the loss is 211697.3294580426, parameters k is -83.33386062959227 and b is 90.79551181362916\n",
      "Iteration 750, the loss is 211659.86947725806, parameters k is -83.32757599520491 and b is 90.79651181362917\n",
      "Iteration 751, the loss is 211622.41281534766, parameters k is -83.32129136081755 and b is 90.79751181362917\n",
      "Iteration 752, the loss is 211584.95947231087, parameters k is -83.3150067264302 and b is 90.79851181362918\n",
      "Iteration 753, the loss is 211547.5094481477, parameters k is -83.30872209204284 and b is 90.79951181362918\n",
      "Iteration 754, the loss is 211510.0627428576, parameters k is -83.30243745765549 and b is 90.80051181362919\n",
      "Iteration 755, the loss is 211472.61935644184, parameters k is -83.29615282326813 and b is 90.80151181362919\n",
      "Iteration 756, the loss is 211435.17928889938, parameters k is -83.28986818888077 and b is 90.8025118136292\n",
      "Iteration 757, the loss is 211397.7425402303, parameters k is -83.28358355449342 and b is 90.8035118136292\n",
      "Iteration 758, the loss is 211360.30911043493, parameters k is -83.27729892010606 and b is 90.8045118136292\n",
      "Iteration 759, the loss is 211322.87899951314, parameters k is -83.2710142857187 and b is 90.80551181362921\n",
      "Iteration 760, the loss is 211285.4522074649, parameters k is -83.26472965133135 and b is 90.80651181362921\n",
      "Iteration 761, the loss is 211248.0287342905, parameters k is -83.258445016944 and b is 90.80751181362922\n",
      "Iteration 762, the loss is 211210.60857998946, parameters k is -83.25216038255664 and b is 90.80851181362922\n",
      "Iteration 763, the loss is 211173.19174456177, parameters k is -83.24587574816928 and b is 90.80951181362923\n",
      "Iteration 764, the loss is 211135.77822800798, parameters k is -83.23959111378193 and b is 90.81051181362923\n",
      "Iteration 765, the loss is 211098.3680303275, parameters k is -83.23330647939457 and b is 90.81151181362924\n",
      "Iteration 766, the loss is 211060.96115152095, parameters k is -83.22702184500722 and b is 90.81251181362924\n",
      "Iteration 767, the loss is 211023.55759158777, parameters k is -83.22073721061986 and b is 90.81351181362925\n",
      "Iteration 768, the loss is 210986.15735052832, parameters k is -83.2144525762325 and b is 90.81451181362925\n",
      "Iteration 769, the loss is 210948.76042834236, parameters k is -83.20816794184515 and b is 90.81551181362926\n",
      "Iteration 770, the loss is 210911.36682502995, parameters k is -83.20188330745779 and b is 90.81651181362926\n",
      "Iteration 771, the loss is 210873.97654059096, parameters k is -83.19559867307044 and b is 90.81751181362927\n",
      "Iteration 772, the loss is 210836.58957502595, parameters k is -83.18931403868308 and b is 90.81851181362927\n",
      "Iteration 773, the loss is 210799.20592833418, parameters k is -83.18302940429572 and b is 90.81951181362928\n",
      "Iteration 774, the loss is 210761.82560051643, parameters k is -83.17674476990837 and b is 90.82051181362928\n",
      "Iteration 775, the loss is 210724.44859157162, parameters k is -83.17046013552101 and b is 90.82151181362929\n",
      "Iteration 776, the loss is 210687.07490150127, parameters k is -83.16417550113366 and b is 90.82251181362929\n",
      "Iteration 777, the loss is 210649.70453030383, parameters k is -83.1578908667463 and b is 90.8235118136293\n",
      "Iteration 778, the loss is 210612.33747797983, parameters k is -83.15160623235894 and b is 90.8245118136293\n",
      "Iteration 779, the loss is 210574.97374452974, parameters k is -83.14532159797159 and b is 90.8255118136293\n",
      "Iteration 780, the loss is 210537.6133299534, parameters k is -83.13903696358423 and b is 90.82651181362931\n",
      "Iteration 781, the loss is 210500.2562342503, parameters k is -83.13275232919688 and b is 90.82751181362931\n",
      "Iteration 782, the loss is 210462.9024574212, parameters k is -83.12646769480952 and b is 90.82851181362932\n",
      "Iteration 783, the loss is 210425.5519994651, parameters k is -83.12018306042216 and b is 90.82951181362932\n",
      "Iteration 784, the loss is 210388.20486038292, parameters k is -83.11389842603481 and b is 90.83051181362933\n",
      "Iteration 785, the loss is 210350.8610401747, parameters k is -83.10761379164745 and b is 90.83151181362933\n",
      "Iteration 786, the loss is 210313.52053883937, parameters k is -83.1013291572601 and b is 90.83251181362934\n",
      "Iteration 787, the loss is 210276.18335637785, parameters k is -83.09504452287274 and b is 90.83351181362934\n",
      "Iteration 788, the loss is 210238.84949279003, parameters k is -83.08875988848538 and b is 90.83451181362935\n",
      "Iteration 789, the loss is 210201.5189480759, parameters k is -83.08247525409803 and b is 90.83551181362935\n",
      "Iteration 790, the loss is 210164.19172223486, parameters k is -83.07619061971067 and b is 90.83651181362936\n",
      "Iteration 791, the loss is 210126.86781526796, parameters k is -83.06990598532332 and b is 90.83751181362936\n",
      "Iteration 792, the loss is 210089.54722717436, parameters k is -83.06362135093596 and b is 90.83851181362937\n",
      "Iteration 793, the loss is 210052.22995795452, parameters k is -83.0573367165486 and b is 90.83951181362937\n",
      "Iteration 794, the loss is 210014.91600760794, parameters k is -83.05105208216125 and b is 90.84051181362938\n",
      "Iteration 795, the loss is 209977.6053761353, parameters k is -83.04476744777389 and b is 90.84151181362938\n",
      "Iteration 796, the loss is 209940.29806353606, parameters k is -83.03848281338654 and b is 90.84251181362939\n",
      "Iteration 797, the loss is 209902.9940698104, parameters k is -83.03219817899918 and b is 90.84351181362939\n",
      "Iteration 798, the loss is 209865.69339495842, parameters k is -83.02591354461183 and b is 90.8445118136294\n",
      "Iteration 799, the loss is 209828.39603898005, parameters k is -83.01962891022447 and b is 90.8455118136294\n",
      "Iteration 800, the loss is 209791.10200187514, parameters k is -83.01334427583711 and b is 90.8465118136294\n",
      "Iteration 801, the loss is 209753.81128364394, parameters k is -83.00705964144976 and b is 90.84751181362941\n",
      "Iteration 802, the loss is 209716.52388428623, parameters k is -83.0007750070624 and b is 90.84851181362941\n",
      "Iteration 803, the loss is 209679.23980380196, parameters k is -82.99449037267505 and b is 90.84951181362942\n",
      "Iteration 804, the loss is 209641.95904219156, parameters k is -82.98820573828769 and b is 90.85051181362942\n",
      "Iteration 805, the loss is 209604.68159945463, parameters k is -82.98192110390033 and b is 90.85151181362943\n",
      "Iteration 806, the loss is 209567.40747559094, parameters k is -82.97563646951298 and b is 90.85251181362943\n",
      "Iteration 807, the loss is 209530.1366706014, parameters k is -82.96935183512562 and b is 90.85351181362944\n",
      "Iteration 808, the loss is 209492.86918448497, parameters k is -82.96306720073827 and b is 90.85451181362944\n",
      "Iteration 809, the loss is 209455.60501724257, parameters k is -82.95678256635091 and b is 90.85551181362945\n",
      "Iteration 810, the loss is 209418.34416887347, parameters k is -82.95049793196355 and b is 90.85651181362945\n",
      "Iteration 811, the loss is 209381.0866393782, parameters k is -82.9442132975762 and b is 90.85751181362946\n",
      "Iteration 812, the loss is 209343.83242875632, parameters k is -82.93792866318884 and b is 90.85851181362946\n",
      "Iteration 813, the loss is 209306.58153700782, parameters k is -82.93164402880149 and b is 90.85951181362947\n",
      "Iteration 814, the loss is 209269.33396413323, parameters k is -82.92535939441413 and b is 90.86051181362947\n",
      "Iteration 815, the loss is 209232.0897101321, parameters k is -82.91907476002677 and b is 90.86151181362948\n",
      "Iteration 816, the loss is 209194.84877500468, parameters k is -82.91279012563942 and b is 90.86251181362948\n",
      "Iteration 817, the loss is 209157.6111587506, parameters k is -82.90650549125206 and b is 90.86351181362949\n",
      "Iteration 818, the loss is 209120.37686137055, parameters k is -82.9002208568647 and b is 90.86451181362949\n",
      "Iteration 819, the loss is 209083.14588286364, parameters k is -82.89393622247735 and b is 90.8655118136295\n",
      "Iteration 820, the loss is 209045.91822323037, parameters k is -82.88765158809 and b is 90.8665118136295\n",
      "Iteration 821, the loss is 209008.69388247043, parameters k is -82.88136695370264 and b is 90.8675118136295\n",
      "Iteration 822, the loss is 208971.47286058456, parameters k is -82.87508231931528 and b is 90.86851181362951\n",
      "Iteration 823, the loss is 208934.25515757242, parameters k is -82.86879768492793 and b is 90.86951181362952\n",
      "Iteration 824, the loss is 208897.0407734335, parameters k is -82.86251305054057 and b is 90.87051181362952\n",
      "Iteration 825, the loss is 208859.8297081681, parameters k is -82.85622841615321 and b is 90.87151181362952\n",
      "Iteration 826, the loss is 208822.62196177637, parameters k is -82.84994378176586 and b is 90.87251181362953\n",
      "Iteration 827, the loss is 208785.41753425825, parameters k is -82.8436591473785 and b is 90.87351181362953\n",
      "Iteration 828, the loss is 208748.21642561373, parameters k is -82.83737451299115 and b is 90.87451181362954\n",
      "Iteration 829, the loss is 208711.0186358428, parameters k is -82.83108987860379 and b is 90.87551181362954\n",
      "Iteration 830, the loss is 208673.8241649457, parameters k is -82.82480524421644 and b is 90.87651181362955\n",
      "Iteration 831, the loss is 208636.63301292146, parameters k is -82.81852060982908 and b is 90.87751181362955\n",
      "Iteration 832, the loss is 208599.44517977125, parameters k is -82.81223597544172 and b is 90.87851181362956\n",
      "Iteration 833, the loss is 208562.26066549474, parameters k is -82.80595134105437 and b is 90.87951181362956\n",
      "Iteration 834, the loss is 208525.079470092, parameters k is -82.79966670666701 and b is 90.88051181362957\n",
      "Iteration 835, the loss is 208487.90159356213, parameters k is -82.79338207227966 and b is 90.88151181362957\n",
      "Iteration 836, the loss is 208450.72703590654, parameters k is -82.7870974378923 and b is 90.88251181362958\n",
      "Iteration 837, the loss is 208413.5557971241, parameters k is -82.78081280350494 and b is 90.88351181362958\n",
      "Iteration 838, the loss is 208376.38787721534, parameters k is -82.77452816911759 and b is 90.88451181362959\n",
      "Iteration 839, the loss is 208339.22327618016, parameters k is -82.76824353473023 and b is 90.88551181362959\n",
      "Iteration 840, the loss is 208302.06199401882, parameters k is -82.76195890034288 and b is 90.8865118136296\n",
      "Iteration 841, the loss is 208264.90403073066, parameters k is -82.75567426595552 and b is 90.8875118136296\n",
      "Iteration 842, the loss is 208227.74938631651, parameters k is -82.74938963156816 and b is 90.8885118136296\n",
      "Iteration 843, the loss is 208190.59806077575, parameters k is -82.74310499718081 and b is 90.88951181362961\n",
      "Iteration 844, the loss is 208153.45005410843, parameters k is -82.73682036279345 and b is 90.89051181362962\n",
      "Iteration 845, the loss is 208116.3053663148, parameters k is -82.7305357284061 and b is 90.89151181362962\n",
      "Iteration 846, the loss is 208079.16399739473, parameters k is -82.72425109401874 and b is 90.89251181362962\n",
      "Iteration 847, the loss is 208042.0259473482, parameters k is -82.71796645963138 and b is 90.89351181362963\n",
      "Iteration 848, the loss is 208004.89121617554, parameters k is -82.71168182524403 and b is 90.89451181362963\n",
      "Iteration 849, the loss is 207967.75980387614, parameters k is -82.70539719085667 and b is 90.89551181362964\n",
      "Iteration 850, the loss is 207930.63171045054, parameters k is -82.69911255646932 and b is 90.89651181362964\n",
      "Iteration 851, the loss is 207893.50693589836, parameters k is -82.69282792208196 and b is 90.89751181362965\n",
      "Iteration 852, the loss is 207856.38548021993, parameters k is -82.6865432876946 and b is 90.89851181362965\n",
      "Iteration 853, the loss is 207819.26734341474, parameters k is -82.68025865330725 and b is 90.89951181362966\n",
      "Iteration 854, the loss is 207782.1525254835, parameters k is -82.67397401891989 and b is 90.90051181362966\n",
      "Iteration 855, the loss is 207745.04102642564, parameters k is -82.66768938453254 and b is 90.90151181362967\n",
      "Iteration 856, the loss is 207707.9328462416, parameters k is -82.66140475014518 and b is 90.90251181362967\n",
      "Iteration 857, the loss is 207670.82798493103, parameters k is -82.65512011575782 and b is 90.90351181362968\n",
      "Iteration 858, the loss is 207633.72644249396, parameters k is -82.64883548137047 and b is 90.90451181362968\n",
      "Iteration 859, the loss is 207596.62821893056, parameters k is -82.64255084698311 and b is 90.90551181362969\n",
      "Iteration 860, the loss is 207559.53331424037, parameters k is -82.63626621259576 and b is 90.90651181362969\n",
      "Iteration 861, the loss is 207522.44172842454, parameters k is -82.6299815782084 and b is 90.9075118136297\n",
      "Iteration 862, the loss is 207485.35346148163, parameters k is -82.62369694382105 and b is 90.9085118136297\n",
      "Iteration 863, the loss is 207448.26851341274, parameters k is -82.61741230943369 and b is 90.9095118136297\n",
      "Iteration 864, the loss is 207411.1868842172, parameters k is -82.61112767504633 and b is 90.91051181362971\n",
      "Iteration 865, the loss is 207374.1085738948, parameters k is -82.60484304065898 and b is 90.91151181362972\n",
      "Iteration 866, the loss is 207337.03358244675, parameters k is -82.59855840627162 and b is 90.91251181362972\n",
      "Iteration 867, the loss is 207299.96190987175, parameters k is -82.59227377188427 and b is 90.91351181362973\n",
      "Iteration 868, the loss is 207262.89355617075, parameters k is -82.58598913749691 and b is 90.91451181362973\n",
      "Iteration 869, the loss is 207225.828521343, parameters k is -82.57970450310955 and b is 90.91551181362973\n",
      "Iteration 870, the loss is 207188.7668053892, parameters k is -82.5734198687222 and b is 90.91651181362974\n",
      "Iteration 871, the loss is 207151.7084083086, parameters k is -82.56713523433484 and b is 90.91751181362974\n",
      "Iteration 872, the loss is 207114.65333010178, parameters k is -82.56085059994749 and b is 90.91851181362975\n",
      "Iteration 873, the loss is 207077.60157076863, parameters k is -82.55456596556013 and b is 90.91951181362975\n",
      "Iteration 874, the loss is 207040.55313030866, parameters k is -82.54828133117277 and b is 90.92051181362976\n",
      "Iteration 875, the loss is 207003.50800872277, parameters k is -82.54199669678542 and b is 90.92151181362976\n",
      "Iteration 876, the loss is 206966.4662060104, parameters k is -82.53571206239806 and b is 90.92251181362977\n",
      "Iteration 877, the loss is 206929.4277221711, parameters k is -82.5294274280107 and b is 90.92351181362977\n",
      "Iteration 878, the loss is 206892.39255720595, parameters k is -82.52314279362335 and b is 90.92451181362978\n",
      "Iteration 879, the loss is 206855.3607111143, parameters k is -82.516858159236 and b is 90.92551181362978\n",
      "Iteration 880, the loss is 206818.33218389598, parameters k is -82.51057352484864 and b is 90.92651181362979\n",
      "Iteration 881, the loss is 206781.30697555118, parameters k is -82.50428889046128 and b is 90.92751181362979\n",
      "Iteration 882, the loss is 206744.28508608008, parameters k is -82.49800425607393 and b is 90.9285118136298\n",
      "Iteration 883, the loss is 206707.2665154829, parameters k is -82.49171962168657 and b is 90.9295118136298\n",
      "Iteration 884, the loss is 206670.2512637589, parameters k is -82.48543498729921 and b is 90.9305118136298\n",
      "Iteration 885, the loss is 206633.23933090866, parameters k is -82.47915035291186 and b is 90.93151181362981\n",
      "Iteration 886, the loss is 206596.23071693192, parameters k is -82.4728657185245 and b is 90.93251181362982\n",
      "Iteration 887, the loss is 206559.22542182892, parameters k is -82.46658108413715 and b is 90.93351181362982\n",
      "Iteration 888, the loss is 206522.22344559935, parameters k is -82.46029644974979 and b is 90.93451181362983\n",
      "Iteration 889, the loss is 206485.22478824362, parameters k is -82.45401181536243 and b is 90.93551181362983\n",
      "Iteration 890, the loss is 206448.22944976093, parameters k is -82.44772718097508 and b is 90.93651181362983\n",
      "Iteration 891, the loss is 206411.23743015225, parameters k is -82.44144254658772 and b is 90.93751181362984\n",
      "Iteration 892, the loss is 206374.24872941698, parameters k is -82.43515791220037 and b is 90.93851181362984\n",
      "Iteration 893, the loss is 206337.26334755533, parameters k is -82.42887327781301 and b is 90.93951181362985\n",
      "Iteration 894, the loss is 206300.2812845674, parameters k is -82.42258864342566 and b is 90.94051181362985\n",
      "Iteration 895, the loss is 206263.30254045292, parameters k is -82.4163040090383 and b is 90.94151181362986\n",
      "Iteration 896, the loss is 206226.32711521216, parameters k is -82.41001937465094 and b is 90.94251181362986\n",
      "Iteration 897, the loss is 206189.35500884487, parameters k is -82.40373474026359 and b is 90.94351181362987\n",
      "Iteration 898, the loss is 206152.38622135107, parameters k is -82.39745010587623 and b is 90.94451181362987\n",
      "Iteration 899, the loss is 206115.4207527312, parameters k is -82.39116547148888 and b is 90.94551181362988\n",
      "Iteration 900, the loss is 206078.45860298455, parameters k is -82.38488083710152 and b is 90.94651181362988\n",
      "Iteration 901, the loss is 206041.4997721118, parameters k is -82.37859620271416 and b is 90.94751181362989\n",
      "Iteration 902, the loss is 206004.54426011225, parameters k is -82.37231156832681 and b is 90.94851181362989\n",
      "Iteration 903, the loss is 205967.59206698625, parameters k is -82.36602693393945 and b is 90.9495118136299\n",
      "Iteration 904, the loss is 205930.6431927343, parameters k is -82.3597422995521 and b is 90.9505118136299\n",
      "Iteration 905, the loss is 205893.6976373555, parameters k is -82.35345766516474 and b is 90.9515118136299\n",
      "Iteration 906, the loss is 205856.7554008506, parameters k is -82.34717303077738 and b is 90.95251181362991\n",
      "Iteration 907, the loss is 205819.81648321918, parameters k is -82.34088839639003 and b is 90.95351181362992\n",
      "Iteration 908, the loss is 205782.88088446122, parameters k is -82.33460376200267 and b is 90.95451181362992\n",
      "Iteration 909, the loss is 205745.94860457684, parameters k is -82.32831912761532 and b is 90.95551181362993\n",
      "Iteration 910, the loss is 205709.01964356634, parameters k is -82.32203449322796 and b is 90.95651181362993\n",
      "Iteration 911, the loss is 205672.09400142942, parameters k is -82.3157498588406 and b is 90.95751181362994\n",
      "Iteration 912, the loss is 205635.17167816553, parameters k is -82.30946522445325 and b is 90.95851181362994\n",
      "Iteration 913, the loss is 205598.2526737757, parameters k is -82.30318059006589 and b is 90.95951181362994\n",
      "Iteration 914, the loss is 205561.33698825946, parameters k is -82.29689595567854 and b is 90.96051181362995\n",
      "Iteration 915, the loss is 205524.42462161646, parameters k is -82.29061132129118 and b is 90.96151181362995\n",
      "Iteration 916, the loss is 205487.51557384737, parameters k is -82.28432668690382 and b is 90.96251181362996\n",
      "Iteration 917, the loss is 205450.6098449518, parameters k is -82.27804205251647 and b is 90.96351181362996\n",
      "Iteration 918, the loss is 205413.7074349297, parameters k is -82.27175741812911 and b is 90.96451181362997\n",
      "Iteration 919, the loss is 205376.80834378133, parameters k is -82.26547278374176 and b is 90.96551181362997\n",
      "Iteration 920, the loss is 205339.91257150643, parameters k is -82.2591881493544 and b is 90.96651181362998\n",
      "Iteration 921, the loss is 205303.02011810534, parameters k is -82.25290351496704 and b is 90.96751181362998\n",
      "Iteration 922, the loss is 205266.1309835776, parameters k is -82.24661888057969 and b is 90.96851181362999\n",
      "Iteration 923, the loss is 205229.2451679233, parameters k is -82.24033424619233 and b is 90.96951181362999\n",
      "Iteration 924, the loss is 205192.36267114262, parameters k is -82.23404961180498 and b is 90.97051181363\n",
      "Iteration 925, the loss is 205155.48349323604, parameters k is -82.22776497741762 and b is 90.97151181363\n",
      "Iteration 926, the loss is 205118.60763420258, parameters k is -82.22148034303027 and b is 90.97251181363\n",
      "Iteration 927, the loss is 205081.73509404296, parameters k is -82.21519570864291 and b is 90.97351181363001\n",
      "Iteration 928, the loss is 205044.8658727568, parameters k is -82.20891107425555 and b is 90.97451181363002\n",
      "Iteration 929, the loss is 205007.9999703441, parameters k is -82.2026264398682 and b is 90.97551181363002\n",
      "Iteration 930, the loss is 204971.13738680482, parameters k is -82.19634180548084 and b is 90.97651181363003\n",
      "Iteration 931, the loss is 204934.2781221395, parameters k is -82.19005717109349 and b is 90.97751181363003\n",
      "Iteration 932, the loss is 204897.42217634767, parameters k is -82.18377253670613 and b is 90.97851181363004\n",
      "Iteration 933, the loss is 204860.56954942946, parameters k is -82.17748790231877 and b is 90.97951181363004\n",
      "Iteration 934, the loss is 204823.72024138484, parameters k is -82.17120326793142 and b is 90.98051181363005\n",
      "Iteration 935, the loss is 204786.87425221325, parameters k is -82.16491863354406 and b is 90.98151181363005\n",
      "Iteration 936, the loss is 204750.03158191612, parameters k is -82.1586339991567 and b is 90.98251181363005\n",
      "Iteration 937, the loss is 204713.19223049204, parameters k is -82.15234936476935 and b is 90.98351181363006\n",
      "Iteration 938, the loss is 204676.3561979419, parameters k is -82.146064730382 and b is 90.98451181363006\n",
      "Iteration 939, the loss is 204639.52348426497, parameters k is -82.13978009599464 and b is 90.98551181363007\n",
      "Iteration 940, the loss is 204602.6940894617, parameters k is -82.13349546160728 and b is 90.98651181363007\n",
      "Iteration 941, the loss is 204565.8680135322, parameters k is -82.12721082721993 and b is 90.98751181363008\n",
      "Iteration 942, the loss is 204529.04525647615, parameters k is -82.12092619283257 and b is 90.98851181363008\n",
      "Iteration 943, the loss is 204492.22581829363, parameters k is -82.11464155844521 and b is 90.98951181363009\n",
      "Iteration 944, the loss is 204455.4096989849, parameters k is -82.10835692405786 and b is 90.99051181363009\n",
      "Iteration 945, the loss is 204418.59689854953, parameters k is -82.1020722896705 and b is 90.9915118136301\n",
      "Iteration 946, the loss is 204381.78741698782, parameters k is -82.09578765528315 and b is 90.9925118136301\n",
      "Iteration 947, the loss is 204344.98125430007, parameters k is -82.08950302089579 and b is 90.9935118136301\n",
      "Iteration 948, the loss is 204308.17841048545, parameters k is -82.08321838650843 and b is 90.99451181363011\n",
      "Iteration 949, the loss is 204271.37888554425, parameters k is -82.07693375212108 and b is 90.99551181363012\n",
      "Iteration 950, the loss is 204234.5826794772, parameters k is -82.07064911773372 and b is 90.99651181363012\n",
      "Iteration 951, the loss is 204197.78979228315, parameters k is -82.06436448334637 and b is 90.99751181363013\n",
      "Iteration 952, the loss is 204161.0002239631, parameters k is -82.05807984895901 and b is 90.99851181363013\n",
      "Iteration 953, the loss is 204124.21397451634, parameters k is -82.05179521457165 and b is 90.99951181363014\n",
      "Iteration 954, the loss is 204087.43104394362, parameters k is -82.0455105801843 and b is 91.00051181363014\n",
      "Iteration 955, the loss is 204050.65143224393, parameters k is -82.03922594579694 and b is 91.00151181363015\n",
      "Iteration 956, the loss is 204013.87513941823, parameters k is -82.03294131140959 and b is 91.00251181363015\n",
      "Iteration 957, the loss is 203977.10216546585, parameters k is -82.02665667702223 and b is 91.00351181363015\n",
      "Iteration 958, the loss is 203940.3325103871, parameters k is -82.02037204263488 and b is 91.00451181363016\n",
      "Iteration 959, the loss is 203903.56617418196, parameters k is -82.01408740824752 and b is 91.00551181363016\n",
      "Iteration 960, the loss is 203866.8031568506, parameters k is -82.00780277386016 and b is 91.00651181363017\n",
      "Iteration 961, the loss is 203830.04345839247, parameters k is -82.00151813947281 and b is 91.00751181363017\n",
      "Iteration 962, the loss is 203793.28707880818, parameters k is -81.99523350508545 and b is 91.00851181363018\n",
      "Iteration 963, the loss is 203756.53401809753, parameters k is -81.9889488706981 and b is 91.00951181363018\n",
      "Iteration 964, the loss is 203719.78427626032, parameters k is -81.98266423631074 and b is 91.01051181363019\n",
      "Iteration 965, the loss is 203683.03785329664, parameters k is -81.97637960192338 and b is 91.0115118136302\n",
      "Iteration 966, the loss is 203646.29474920678, parameters k is -81.97009496753603 and b is 91.0125118136302\n",
      "Iteration 967, the loss is 203609.5549639902, parameters k is -81.96381033314867 and b is 91.0135118136302\n",
      "Iteration 968, the loss is 203572.8184976476, parameters k is -81.95752569876132 and b is 91.01451181363021\n",
      "Iteration 969, the loss is 203536.085350178, parameters k is -81.95124106437396 and b is 91.01551181363021\n",
      "Iteration 970, the loss is 203499.35552158247, parameters k is -81.9449564299866 and b is 91.01651181363022\n",
      "Iteration 971, the loss is 203462.62901186015, parameters k is -81.93867179559925 and b is 91.01751181363022\n",
      "Iteration 972, the loss is 203425.9058210116, parameters k is -81.93238716121189 and b is 91.01851181363023\n",
      "Iteration 973, the loss is 203389.18594903676, parameters k is -81.92610252682454 and b is 91.01951181363023\n",
      "Iteration 974, the loss is 203352.46939593542, parameters k is -81.91981789243718 and b is 91.02051181363024\n",
      "Iteration 975, the loss is 203315.75616170763, parameters k is -81.91353325804982 and b is 91.02151181363024\n",
      "Iteration 976, the loss is 203279.04624635365, parameters k is -81.90724862366247 and b is 91.02251181363025\n",
      "Iteration 977, the loss is 203242.33964987303, parameters k is -81.90096398927511 and b is 91.02351181363025\n",
      "Iteration 978, the loss is 203205.6363722661, parameters k is -81.89467935488776 and b is 91.02451181363026\n",
      "Iteration 979, the loss is 203168.9364135325, parameters k is -81.8883947205004 and b is 91.02551181363026\n",
      "Iteration 980, the loss is 203132.23977367283, parameters k is -81.88211008611304 and b is 91.02651181363026\n",
      "Iteration 981, the loss is 203095.5464526864, parameters k is -81.87582545172569 and b is 91.02751181363027\n",
      "Iteration 982, the loss is 203058.85645057374, parameters k is -81.86954081733833 and b is 91.02851181363027\n",
      "Iteration 983, the loss is 203022.16976733453, parameters k is -81.86325618295098 and b is 91.02951181363028\n",
      "Iteration 984, the loss is 202985.48640296896, parameters k is -81.85697154856362 and b is 91.03051181363028\n",
      "Iteration 985, the loss is 202948.80635747698, parameters k is -81.85068691417626 and b is 91.03151181363029\n",
      "Iteration 986, the loss is 202912.1296308589, parameters k is -81.84440227978891 and b is 91.0325118136303\n",
      "Iteration 987, the loss is 202875.45622311413, parameters k is -81.83811764540155 and b is 91.0335118136303\n",
      "Iteration 988, the loss is 202838.7861342429, parameters k is -81.8318330110142 and b is 91.0345118136303\n",
      "Iteration 989, the loss is 202802.1193642451, parameters k is -81.82554837662684 and b is 91.03551181363031\n",
      "Iteration 990, the loss is 202765.4559131214, parameters k is -81.81926374223949 and b is 91.03651181363031\n",
      "Iteration 991, the loss is 202728.7957808709, parameters k is -81.81297910785213 and b is 91.03751181363032\n",
      "Iteration 992, the loss is 202692.13896749372, parameters k is -81.80669447346477 and b is 91.03851181363032\n",
      "Iteration 993, the loss is 202655.48547299084, parameters k is -81.80040983907742 and b is 91.03951181363033\n",
      "Iteration 994, the loss is 202618.8352973609, parameters k is -81.79412520469006 and b is 91.04051181363033\n",
      "Iteration 995, the loss is 202582.1884406051, parameters k is -81.7878405703027 and b is 91.04151181363034\n",
      "Iteration 996, the loss is 202545.54490272223, parameters k is -81.78155593591535 and b is 91.04251181363034\n",
      "Iteration 997, the loss is 202508.9046837134, parameters k is -81.775271301528 and b is 91.04351181363035\n",
      "Iteration 998, the loss is 202472.26778357784, parameters k is -81.76898666714064 and b is 91.04451181363035\n",
      "Iteration 999, the loss is 202435.63420231603, parameters k is -81.76270203275328 and b is 91.04551181363036\n"
     ]
    }
   ],
   "source": [
    "k = random.random() * 200 - 100  # -100 100\n",
    "b = random.random() * 200 - 100  # -100 100\n",
    "\n",
    "learning_rate = 1e-3\n",
    "\n",
    "iteration_num = 1000 \n",
    "losses = []\n",
    "for i in range(iteration_num):\n",
    "    \n",
    "    price_use_current_parameters = [price(r, k, b) for r in X_rm]  # \\hat{y}\n",
    "    \n",
    "    current_loss = loss(y, price_use_current_parameters)\n",
    "    losses.append(current_loss)\n",
    "    print(\"Iteration {}, the loss is {}, parameters k is {} and b is {}\".format(i,current_loss,k,b))\n",
    "    \n",
    "    k_gradient = partial_derivative_k(X_rm) #带入新的斜率\n",
    "    b_gradient = partial_derivative_b() #带入新的截距\n",
    "    \n",
    "    k = k + (-1 * k_gradient) * learning_rate\n",
    "    b = b + (-1 * b_gradient) * learning_rate\n",
    "best_k = k\n",
    "best_b = b"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<评阅点>\n",
    "+ 是否将Loss改成了“绝对值”(3')\n",
    "+ 是否完成了偏导的重新定义(5')\n",
    "+ 新的模型Loss是否能够收敛 (11’)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
