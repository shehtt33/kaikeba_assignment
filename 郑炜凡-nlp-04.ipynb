{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Assignment 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. 复习上课内容以及复现课程代码"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "在本部分，你需要复习上课内容和课程代码后，自己复现课程代码。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import icecream as ic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "'''\n",
    "我们自己写几个节点（神经元)\n",
    "\n",
    "'''\n",
    "\n",
    "class Node:\n",
    "    # 该类为所有其他图节点类的父类\n",
    "    def __init__(self, inputs=[]):\n",
    "        #定义每个节点的输入和输出\n",
    "        self.inputs = inputs\n",
    "        self.outputs = []\n",
    "        \n",
    "        #每个节点都是其输入节点的输出节点\n",
    "        for n in self.inputs: \n",
    "            n.outputs.append(self)\n",
    "            # set 'self' node as inbound_nodes's outbound_nodes\n",
    "\n",
    "        self.value = None\n",
    "        \n",
    "        self.gradients = {} #保存梯度\n",
    "        # keys are the inputs to this node, and their\n",
    "        # values are the partials of this node with \n",
    "        # respect to that input.\n",
    "        # \\partial{node}{input_i}\n",
    "        \n",
    "\n",
    "    def forward(self):\n",
    "        #前向传播函数 继承该类的其他类会覆写该函数\n",
    "        '''\n",
    "        Forward propagation. \n",
    "        Compute the output value vased on 'inbound_nodes' and store the \n",
    "        result in self.value\n",
    "        '''\n",
    "\n",
    "        raise NotImplemented\n",
    "    \n",
    "\n",
    "    def backward(self):\n",
    "        #反向传播函数，继承该类的其他类会覆写该函数\n",
    "\n",
    "        raise NotImplemented\n",
    "        \n",
    "class Input(Node):\n",
    "    # 输入节点，包括神经网络输入节点，权重节点，和偏差节点\n",
    "    def __init__(self):\n",
    "        '''\n",
    "        An Input node has no inbound nodes.\n",
    "        So no need to pass anything to the Node instantiator.\n",
    "        '''\n",
    "        Node.__init__(self)\n",
    "\n",
    "    def forward(self, value=None):\n",
    "        '''\n",
    "        Only input node is the node where the value may be passed\n",
    "        as an argument to forward().\n",
    "        All other node implementations should get the value of the \n",
    "        previous node from self.inbound_nodes\n",
    "        \n",
    "        Example: \n",
    "        val0: self.inbound_nodes[0].value\n",
    "        '''\n",
    "        #定义节点数值\n",
    "        if value is not None:\n",
    "            self.value = value\n",
    "            ## It's is input node, when need to forward, this node initiate self's value.\n",
    "\n",
    "        # Input subclass just holds a value, such as a data feature or a model parameter(weight/bias)\n",
    "        \n",
    "    def backward(self):\n",
    "        #计算节点梯度\n",
    "        self.gradients = {self:0} # initialization \n",
    "        for n in self.outputs:\n",
    "            #以下计算该节点的输出节点对该节点的梯度\n",
    "            grad_cost = n.gradients[self] \n",
    "            self.gradients[self] = grad_cost * 1\n",
    "            \n",
    "        \n",
    "        # input N --> N1, N2\n",
    "        # \\partial L / \\partial N \n",
    "        # ==> \\partial L / \\partial N1 * \\ partial N1 / \\partial N\n",
    "\n",
    "\n",
    "class Add(Node):\n",
    "    def __init__(self, *nodes):\n",
    "        Node.__init__(self, nodes)\n",
    "\n",
    "\n",
    "    def forward(self):\n",
    "        self.value = sum(map(lambda n: n.value, self.inputs))\n",
    "        ## when execute forward, this node caculate value as defined.\n",
    "\n",
    "class Linear(Node):\n",
    "    #全连接网络层的计算\n",
    "    def __init__(self, nodes, weights, bias):\n",
    "        Node.__init__(self, [nodes, weights, bias])\n",
    "\n",
    "    def forward(self):\n",
    "        #前向传播计算 y = w*x + b\n",
    "        inputs = self.inputs[0].value\n",
    "        weights = self.inputs[1].value\n",
    "        bias = self.inputs[2].value\n",
    "\n",
    "        self.value = np.dot(inputs, weights) + bias\n",
    "        \n",
    "    def backward(self):\n",
    "        #反向传播计算\n",
    "        # initial a partial for each of the inbound_nodes.\n",
    "        self.gradients = {n: np.zeros_like(n.value) for n in self.inputs}\n",
    "\n",
    "        for n in self.outputs:\n",
    "            # Get the partial of the cost w.r.t this node.\n",
    "            grad_cost = n.gradients[self]\n",
    "            #以下分别计算对inputs， weights, bias的梯度\n",
    "            self.gradients[self.inputs[0]] = np.dot(grad_cost, self.inputs[1].value.T)\n",
    "            self.gradients[self.inputs[1]] = np.dot(self.inputs[0].value.T, grad_cost)\n",
    "            self.gradients[self.inputs[2]] = np.sum(grad_cost, axis=0, keepdims=False)\n",
    "\n",
    "        # WX + B / W ==> X\n",
    "        # WX + B / X ==> W\n",
    "\n",
    "class Sigmoid(Node):\n",
    "    #定义sigmoid函数\n",
    "    def __init__(self, node):\n",
    "        Node.__init__(self, [node])\n",
    "\n",
    "\n",
    "    def _sigmoid(self, x):\n",
    "        return 1./(1 + np.exp(-1 * x))\n",
    "\n",
    "    def forward(self):\n",
    "        #前向 即为sigmoid函数计算\n",
    "        self.x = self.inputs[0].value   # [0] input is a list\n",
    "        self.value = self._sigmoid(self.x)\n",
    "\n",
    "    def backward(self):\n",
    "        #反向传播计算梯度\n",
    "        self.partial = self._sigmoid(self.x) * (1 - self._sigmoid(self.x))\n",
    "        \n",
    "        # y = 1 / (1 + e^-x)\n",
    "        # y' = 1 / (1 + e^-x) (1 - 1 / (1 + e^-x))\n",
    "        \n",
    "        self.gradients = {n: np.zeros_like(n.value) for n in self.inputs}\n",
    "\n",
    "        for n in self.outputs:\n",
    "            grad_cost = n.gradients[self]  # Get the partial of the cost with respect to this node.\n",
    "\n",
    "            self.gradients[self.inputs[0]] = grad_cost * self.partial\n",
    "            # use * to keep all the dimension same!.\n",
    "\n",
    "\n",
    "\n",
    "class MSE(Node):\n",
    "    # 定义平均平方误差\n",
    "    def __init__(self, y, a):\n",
    "        Node.__init__(self, [y, a])\n",
    "\n",
    "\n",
    "    def forward(self):\n",
    "        #前向传播计算\n",
    "        y = self.inputs[0].value.reshape(-1, 1)\n",
    "        a = self.inputs[1].value.reshape(-1, 1)\n",
    "        assert(y.shape == a.shape)\n",
    "\n",
    "        self.m = self.inputs[0].value.shape[0]\n",
    "        self.diff = y - a\n",
    "\n",
    "        self.value = np.mean(self.diff**2)\n",
    "\n",
    "\n",
    "    def backward(self):\n",
    "        #反向计算相应梯度\n",
    "        self.gradients[self.inputs[0]] = (2 / self.m) * self.diff\n",
    "        self.gradients[self.inputs[1]] = (-2 / self.m) * self.diff\n",
    "\n",
    "\n",
    "def forward_and_backward(outputnode, graph):\n",
    "    # execute all the forward method of sorted_nodes.\n",
    "\n",
    "    ## In practice, it's common to feed in mutiple data example in each forward pass rather than just 1. Because the examples can be processed in parallel. The number of examples is called batch size.\n",
    "    for n in graph:\n",
    "        n.forward()\n",
    "        ## each node execute forward, get self.value based on the topological sort result.\n",
    "\n",
    "    for n in  graph[::-1]:\n",
    "        n.backward()\n",
    "\n",
    "    #return outputnode.value\n",
    "\n",
    "###   v -->  a -->  C\n",
    "##    b --> C\n",
    "##    b --> v -- a --> C\n",
    "##    v --> v ---> a -- > C\n",
    "\n",
    "def topological_sort(feed_dict):\n",
    "    \"\"\"\n",
    "    Sort generic nodes in topological order using Kahn's Algorithm.\n",
    "    `feed_dict`: A dictionary where the key is a `Input` node and the value is the respective value feed to that node.\n",
    "    Returns a list of sorted nodes.\n",
    "    \"\"\"\n",
    "\n",
    "    input_nodes = [n for n in feed_dict.keys()]\n",
    "\n",
    "    G = {}\n",
    "    nodes = [n for n in input_nodes]\n",
    "    while len(nodes) > 0:\n",
    "        n = nodes.pop(0)\n",
    "        if n not in G:\n",
    "            G[n] = {'in': set(), 'out': set()}\n",
    "        for m in n.outputs:\n",
    "            if m not in G:\n",
    "                G[m] = {'in': set(), 'out': set()}\n",
    "            G[n]['out'].add(m)\n",
    "            G[m]['in'].add(n)\n",
    "            nodes.append(m)\n",
    "\n",
    "    L = []\n",
    "    S = set(input_nodes)\n",
    "    while len(S) > 0:\n",
    "        n = S.pop()\n",
    "\n",
    "        if isinstance(n, Input):\n",
    "            n.value = feed_dict[n]\n",
    "            ## if n is Input Node, set n'value as \n",
    "            ## feed_dict[n]\n",
    "            ## else, n's value is caculate as its\n",
    "            ## inbounds\n",
    "\n",
    "        L.append(n)\n",
    "        for m in n.outputs:\n",
    "            G[n]['out'].remove(m)\n",
    "            G[m]['in'].remove(n)\n",
    "            # if no other incoming edges add to S\n",
    "            if len(G[m]['in']) == 0:\n",
    "                S.add(m)\n",
    "    return L\n",
    "\n",
    "#随机梯度下降\n",
    "def sgd_update(trainables, learning_rate=1e-2):\n",
    "    # there are so many other update / optimization methods\n",
    "    # such as Adam, Mom, \n",
    "    for t in trainables:\n",
    "        t.value += -1 * learning_rate * t.gradients[t]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_boston #导入波士顿房价"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = load_boston"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "losses = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of examples = 506\n",
      "Epoch: 1, Loss: 142.371\n",
      "Epoch: 101, Loss: 6.925\n",
      "Epoch: 201, Loss: 5.169\n",
      "Epoch: 301, Loss: 4.876\n",
      "Epoch: 401, Loss: 4.576\n",
      "Epoch: 501, Loss: 4.148\n",
      "Epoch: 601, Loss: 3.919\n",
      "Epoch: 701, Loss: 4.046\n",
      "Epoch: 801, Loss: 3.460\n",
      "Epoch: 901, Loss: 3.907\n",
      "Epoch: 1001, Loss: 3.494\n",
      "Epoch: 1101, Loss: 3.317\n",
      "Epoch: 1201, Loss: 3.775\n",
      "Epoch: 1301, Loss: 2.745\n",
      "Epoch: 1401, Loss: 3.476\n",
      "Epoch: 1501, Loss: 3.070\n",
      "Epoch: 1601, Loss: 3.412\n",
      "Epoch: 1701, Loss: 2.775\n",
      "Epoch: 1801, Loss: 2.636\n",
      "Epoch: 1901, Loss: 3.612\n",
      "Epoch: 2001, Loss: 3.661\n",
      "Epoch: 2101, Loss: 3.403\n",
      "Epoch: 2201, Loss: 3.190\n",
      "Epoch: 2301, Loss: 2.851\n",
      "Epoch: 2401, Loss: 3.182\n",
      "Epoch: 2501, Loss: 2.914\n",
      "Epoch: 2601, Loss: 2.993\n",
      "Epoch: 2701, Loss: 3.271\n",
      "Epoch: 2801, Loss: 3.710\n",
      "Epoch: 2901, Loss: 3.212\n",
      "Epoch: 3001, Loss: 3.264\n",
      "Epoch: 3101, Loss: 3.047\n",
      "Epoch: 3201, Loss: 2.939\n",
      "Epoch: 3301, Loss: 3.148\n",
      "Epoch: 3401, Loss: 2.850\n",
      "Epoch: 3501, Loss: 3.477\n",
      "Epoch: 3601, Loss: 2.830\n",
      "Epoch: 3701, Loss: 2.878\n",
      "Epoch: 3801, Loss: 3.384\n",
      "Epoch: 3901, Loss: 2.548\n",
      "Epoch: 4001, Loss: 3.164\n",
      "Epoch: 4101, Loss: 2.981\n",
      "Epoch: 4201, Loss: 3.048\n",
      "Epoch: 4301, Loss: 2.952\n",
      "Epoch: 4401, Loss: 2.933\n",
      "Epoch: 4501, Loss: 2.937\n",
      "Epoch: 4601, Loss: 3.206\n",
      "Epoch: 4701, Loss: 2.468\n",
      "Epoch: 4801, Loss: 2.995\n",
      "Epoch: 4901, Loss: 2.934\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Check out the new network architecture and dataset!\n",
    "Notice that the weights and biases are\n",
    "generated randomly.\n",
    "No need to change anything, but feel free to tweak\n",
    "to test your network, play around with the epochs, batch size, etc!\n",
    "\"\"\"\n",
    "\n",
    "import numpy as np\n",
    "from sklearn.datasets import load_boston\n",
    "from sklearn.utils import shuffle, resample\n",
    "#from miniflow import *\n",
    "\n",
    "# Load data\n",
    "data = load_boston()\n",
    "X_ = data['data']\n",
    "y_ = data['target']\n",
    "\n",
    "# Normalize data\n",
    "X_ = (X_ - np.mean(X_, axis=0)) / np.std(X_, axis=0)\n",
    "\n",
    "n_features = X_.shape[1]\n",
    "n_hidden = 10\n",
    "W1_ = np.random.randn(n_features, n_hidden)\n",
    "b1_ = np.zeros(n_hidden)\n",
    "W2_ = np.random.randn(n_hidden, 1)\n",
    "b2_ = np.zeros(1)\n",
    "\n",
    "# Neural network\n",
    "X, y = Input(), Input()\n",
    "W1, b1 = Input(), Input()\n",
    "W2, b2 = Input(), Input()\n",
    "\n",
    "l1 = Linear(X, W1, b1)\n",
    "s1 = Sigmoid(l1)\n",
    "l2 = Linear(s1, W2, b2)\n",
    "cost = MSE(y, l2)\n",
    "\n",
    "feed_dict = {\n",
    "    X: X_,\n",
    "    y: y_,\n",
    "    W1: W1_,\n",
    "    b1: b1_,\n",
    "    W2: W2_,\n",
    "    b2: b2_\n",
    "}\n",
    "\n",
    "epochs = 5000\n",
    "# Total number of examples\n",
    "m = X_.shape[0]\n",
    "batch_size = 16\n",
    "steps_per_epoch = m // batch_size\n",
    "\n",
    "graph = topological_sort(feed_dict)\n",
    "trainables = [W1, b1, W2, b2]\n",
    "\n",
    "print(\"Total number of examples = {}\".format(m))\n",
    "\n",
    "# Step 4\n",
    "for i in range(epochs):\n",
    "    loss = 0\n",
    "    for j in range(steps_per_epoch):\n",
    "        # Step 1\n",
    "        # Randomly sample a batch of examples\n",
    "        X_batch, y_batch = resample(X_, y_, n_samples=batch_size)\n",
    "\n",
    "        # Reset value of X and y Inputs\n",
    "        X.value = X_batch\n",
    "        y.value = y_batch\n",
    "\n",
    "        # Step 2\n",
    "        _ = None\n",
    "        forward_and_backward(_, graph) # set output node not important.\n",
    "\n",
    "        # Step 3\n",
    "        rate = 1e-2\n",
    "    \n",
    "        sgd_update(trainables, rate)\n",
    "\n",
    "        loss += graph[-1].value\n",
    "    \n",
    "    if i % 100 == 0: \n",
    "        print(\"Epoch: {}, Loss: {:.3f}\".format(i+1, loss/steps_per_epoch))\n",
    "        losses.append(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def forward(outputNode,graph):\n",
    "    for n in graph:\n",
    "        n.forward()\n",
    "    return outputNode.value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[19.53348551],\n",
       "       [13.1163174 ],\n",
       "       [13.76477341],\n",
       "       [18.69312857],\n",
       "       [18.27891266],\n",
       "       [10.22924339],\n",
       "       [24.79712952],\n",
       "       [16.31318165],\n",
       "       [22.26771771],\n",
       "       [16.09884658],\n",
       "       [14.8749335 ],\n",
       "       [22.24932533],\n",
       "       [17.22924396],\n",
       "       [15.07231739],\n",
       "       [ 8.97211812],\n",
       "       [20.0311985 ]])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "forward(l2,graph)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x13df56e3e48>]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(range(len(losses)), losses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 7.61056209],\n",
       "       [ 6.88035257],\n",
       "       [14.46272917],\n",
       "       [ 4.61805542],\n",
       "       [ 8.59266485],\n",
       "       [ 4.26983725],\n",
       "       [-8.28719468],\n",
       "       [ 6.90892213],\n",
       "       [ 7.41228813],\n",
       "       [15.14992242]])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "W2.value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_ = data['data']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([6.320e-03, 1.800e+01, 2.310e+00, 0.000e+00, 5.380e-01, 6.575e+00,\n",
       "       6.520e+01, 4.090e+00, 1.000e+00, 2.960e+02, 1.530e+01, 3.969e+02,\n",
       "       4.980e+00])"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (<ipython-input-22-fadf4753258a>, line 1)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  File \u001b[1;32m\"<ipython-input-22-fadf4753258a>\"\u001b[1;36m, line \u001b[1;32m1\u001b[0m\n\u001b[1;33m    conda install keras\u001b[0m\n\u001b[1;37m                ^\u001b[0m\n\u001b[1;31mSyntaxError\u001b[0m\u001b[1;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "import keras #还没装\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'keras'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-20-166ed32f3d51>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[0mkeras\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlayers\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mDense\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mkeras\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmodels\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mSequential\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[0mmodel\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mSequential\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'keras'"
     ]
    }
   ],
   "source": [
    "from keras.layers import Dense\n",
    "from keras.models import Sequential\n",
    "\n",
    "model = Sequential()\n",
    "\n",
    "model.add(Dense(units=64, activation='sigmoid', input_dim=13))\n",
    "model.add(Dense(units=30, activation='sigmoid', input_dim=64))\n",
    "model.add(Dense(units=1))\n",
    "\n",
    "model.compile(loss='mse',\n",
    "              optimizer='sgd',\n",
    "              metrics=['mse'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. 回答一下理论题目"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "#### 1. What does a neuron compute?\n",
    "\n",
    "答案:在我看来，神经元是数学模型，包括了input + function + output\n",
    "                           输入 + 函数计算 + 输出\n",
    "    多个神经元相连接便形成了神经网络。"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "####  2. Why we use non-linear activation funcitons in neural networks?\n",
    "答案: 线性的激活函数可以用w'x + b'来表示，那么多层神经网络就没有意义了。\n",
    "    无论神经网络有多少层，一直在做的只是计算线性激活函数，还不如去掉隐藏层。"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "#### 3. What is the 'Logistic Loss' ?\n",
    "答案:即损失函数，也叫误差函数，是真实值和预测值之间的差。表示为L(y_hat, y)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "#### 4. Assume that you are building a binary classifier for detecting if an image containing cats, which activation functions would you recommen using for the output layer ?\n",
    "\n",
    "A. ReLU    \n",
    "B. Leaky ReLU    \n",
    "C. sigmoid    \n",
    "D. tanh  \n",
    "\n",
    "答案：C sigmoid，首先上述四个函数都是非线性函数，都可以作为激活函数。而检测猫是一个二分     类问题，可以看作1是猫，2不是猫。那么sigmoid函数是平滑地从0渐变到1，且每个点的\n",
    "    斜率都不相同，很适合用来做二分类的问题。"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "#### 5. Why we don't use zero initialization for all parameters ?\n",
    "\n",
    "答案：如果使用全零初始化，会出现神经网络的对称性问题。向前传播和反向传播，每个神经元的值\n",
    "    都一样。一般我们选择随机初始化，就不存在对称性问题了。"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "#### 6. Can you implement the softmax function using python ? \n",
    "答案：def softmax:\n",
    "    s = np.exp(e)/np.sum(np.exp(e))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.实践题"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### In this practical part, you will build a simple digits recognizer to check if the digit in the image is larger than 5. This assignmnet will guide you step by step to finish your first small project in this course ."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1 - Packages  \n",
    "sklearn is a famous package for machine learning.   \n",
    "matplotlib is a common package for vasualization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import datasets\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2 - Overvie of the dataset  \n",
    "    - a training set has m_train images labeled as 0 if the digit < 5 or 1 if the digit >= 5\n",
    "    - a test set contains m_test images labels as if the digit < 5 or 1 if the digit >= 5\n",
    "    - eah image if of shape (num_px, num_px ). Thus, each image is square(height=num_px and  width = num_px)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading the data \n",
    "digits = datasets.load_digits()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 10 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Vilizating the data\n",
    "for i in range(1,11):\n",
    "    plt.subplot(2,5,i)\n",
    "    plt.imshow(digits.data[i-1].reshape([8,8]),cmap=plt.cm.gray_r)\n",
    "    plt.text(3,10,str(digits.target[i-1]))\n",
    "    plt.xticks([])\n",
    "    plt.yticks([])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the data into training set and test set \n",
    "X_train, X_test, y_train, y_test = train_test_split(digits.data, digits.target, test_size=0.25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# reformulate the label. \n",
    "# If the digit is smaller than 5, the label is 0.\n",
    "# If the digit is larger than 5, the label is 1.\n",
    "\n",
    "y_train[y_train < 5 ] = 0\n",
    "y_train[y_train >= 5] = 1\n",
    "y_test[y_test < 5] = 0\n",
    "y_test[y_test >= 5] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1347, 64)\n",
      "(450, 64)\n",
      "(1347,)\n",
      "(450,)\n"
     ]
    }
   ],
   "source": [
    "print(X_train.shape)\n",
    "print(X_test.shape)\n",
    "print(y_train.shape)\n",
    "print(y_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3- Architecture of the neural network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![title](./networks.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/limingxiao/Desktop/NLP_Lectures/jupyters_and_slides/2019-spring\r\n"
     ]
    }
   ],
   "source": [
    "!pwd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Mathematical expression of the algorithm:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For one example $x^{(i)}$:   \n",
    " $$ z^{(i)} = w^T * x^{(i)} +b $$   \n",
    " $$ y^{(i)} = a^{(i)} = sigmoid(z^{(i)})$$   \n",
    " $$L(a^{(i)},y^{(i)}) = -y^{(i)} log(a^{(i)})-(1-y^{(i)})log(1-a^{(i)})$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The total cost over all training examples:\n",
    "$$ J = \\frac{1}{m}\\sum_{i=1}^{m}L(a^{(i)},y^{(i)}) $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4 - Building the algorithm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 4.1- Activation function    \n",
    "###### Exercise:\n",
    "Finish the sigmoid funciton "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(z):\n",
    "    '''\n",
    "    Compute the sigmoid of z\n",
    "    Arguments: z -- a scalar or numpy array of any size.\n",
    "    \n",
    "    Return:\n",
    "    s -- sigmoid(z)\n",
    "    '''\n",
    "    s = 1. / (1 + np.exp(-1 * z))\n",
    "    \n",
    "    return s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sigmoid([0,2]) = [0.5        0.88079708]\n"
     ]
    }
   ],
   "source": [
    "# Test your code \n",
    "# The result should be [0.5 0.88079708]\n",
    "print(\"sigmoid([0,2]) = \" + str(sigmoid(np.array([0,2]))))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 4.1-Initializaing parameters\n",
    "###### Exercise:\n",
    "Finishe the initialize_parameters function below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Random innitialize the parameters\n",
    "\n",
    "def initialize_parameters(dim):\n",
    "    '''\n",
    "    Argument: dim -- size of the w vector\n",
    "    \n",
    "    Returns:\n",
    "    w -- initialized vector of shape (dim,1)\n",
    "    b -- initializaed scalar\n",
    "    '''\n",
    "    \n",
    "    w = np.random.randn(dim,1)\n",
    "    b = np.random.randint(1)\n",
    "    assert(w.shape == (dim,1))\n",
    "    assert(isinstance(b,float) or isinstance(b,int))\n",
    "    \n",
    "    return w,b"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4.3-Forward and backward propagation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Some mathematical expressions\n",
    "Forward Propagation:   \n",
    ". X    \n",
    ". A = $\\sigma(w^T*X+b) = (a^{(1)},a^{(2)},...,a^{(m)}$   \n",
    ". J = $-\\frac{1}{m} \\sum_{i=1}^{m}y^{(i)}log(a^{(i)}+(1-y^{(i)})log(1-a^{(i)})$       "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Some derivative: \n",
    "$$\\frac{\\partial{J}}{\\partial{w}} = \\frac{1}{m}X*(A-Y)^T$$   \n",
    "$$\\frac{\\partial{J}}{\\partial{b}} = \\frac{1}{m}\\sum_{i=1}^m(a^{(i)}-y^{(i)}) $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Exercise:\n",
    "Finish the function below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def propagate(w,b,X,Y):\n",
    "    '''\n",
    "    Implement the cost function and its gradient for the propagation\n",
    "    \n",
    "    Arguments:\n",
    "    w - weights\n",
    "    b - bias\n",
    "    X - data\n",
    "    Y - ground truth\n",
    "    '''\n",
    "    m = X.shape[1]\n",
    "    z = np.dot(X,w) + b\n",
    "    A = sigmoid(z)\n",
    "    cost = -1 /m * np.sum(Y_r + np.log(A+1-Y_r) * np.log(1-Y_r))\n",
    "    \n",
    "    dw = np.dot(X.T,(A-Y_r)) / m\n",
    "    db = np.sum(A-Y_r) / m\n",
    "    assert(dw.shape == w.shape)\n",
    "    assert(db.dtype == float)\n",
    "    cost = np.squeeze(cost)\n",
    "    assert(cost.shape == ())\n",
    "    \n",
    "    grads = {'dw':dw,\n",
    "             'db':db}\n",
    "    return grads, cost"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 4.4 -Optimization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Exercise:\n",
    "Minimizing the cost function using gradient descent.   \n",
    "$$\\theta = \\theta - \\alpha*d\\theta$$ where $\\alpha$ is the learning rate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [],
   "source": [
    "def optimize(w, b, X, Y, num_iterations, learning_rate, print_cost=False):\n",
    "    '''\n",
    "    This function optimize w and b by running a gradient descen algorithm\n",
    "    \n",
    "    Arguments:\n",
    "    w - weights\n",
    "    b - bias\n",
    "    X - data\n",
    "    Y - ground truth\n",
    "    num_iterations -- number of iterations of the optimization loop\n",
    "    learning_rate -- learning rate of the gradient descent update rule\n",
    "    print_cost -- True to print the loss every 100 steps\n",
    "    \n",
    "    Returns:\n",
    "    params - dictionary containing the weights w and bias b\n",
    "    grads -- dictionary containing the gradients of the weights and bias with respect to the cost function\n",
    "    costs -- list of all the costs computed during the optimization, this will be used to plot the learning curve.\n",
    "    \n",
    "    '''\n",
    "    \n",
    "    costs = []\n",
    "    \n",
    "    for i in range(num_iterations):\n",
    "        \n",
    "        grads, cost = None\n",
    "        \n",
    "        dw = grads['dw']\n",
    "        db = grads['db']\n",
    "        \n",
    "        w = None\n",
    "        b = None\n",
    "        \n",
    "        if i % 100 == 0:\n",
    "            costs.append(cost)\n",
    "        if print_cost and i % 100 == 0:\n",
    "            print (\"Cost after iteration %i: %f\" %(i, cost))\n",
    "    \n",
    "    params = {\"w\":w,\n",
    "              \"b\":b}\n",
    "    \n",
    "    grads = {\"dw\":dw,\n",
    "             \"db\":db}\n",
    "    \n",
    "    return params, grads, costs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Exercise\n",
    "The previous function will output the learned w and b. We are able to use w and b to predict the labels for a dataset X. Implement the predict() function.    \n",
    "Two steps to finish this task:   \n",
    "1. Calculate $\\hat{Y} = A = \\sigma(w^T*X+b)$   \n",
    "2. Convert the entries of a into 0 (if activation <= 0.5) or 1 (if activation > 0.5), stores the predictions in a vector Y_prediction. If you wish, you can use an if/else statement in a for loop (though there is also a way to vectorize this)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(w, b, X):\n",
    "    '''\n",
    "    Predict whether the label is 0 or 1 using learned logistic regression parameters (w, b)\n",
    "    \n",
    "    Arguments:\n",
    "    w -- weights\n",
    "    b -- bias \n",
    "    X -- data \n",
    "    \n",
    "    Returns:\n",
    "    Y_prediction -- a numpy array (vector) containing all predictions (0/1) for the examples in X\n",
    "    '''\n",
    "    m = X.shape[1]\n",
    "    Y_prediction = np.zeros((1,m))\n",
    "    w = w.reshape(X.shape[0],1)\n",
    "    \n",
    "    A = None\n",
    "    \n",
    "    for i in range(A.shape[i]):\n",
    "        None \n",
    "    \n",
    "    assert(Y_prediction.shape == (1,m))\n",
    "    \n",
    "    return Y_prediction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 5- Merge all functions into a model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Congratulations !! You have finished all the necessary components for constructing a model. Now, Let's take the challenge to merge all the implemented function into one model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model(X_train, Y_trein, X_test, Y_test, num_iterations, learning_rate,print_cost):\n",
    "    \"\"\"\n",
    "    Build the logistic regression model by calling all the functions you have implemented.\n",
    "    Arguments:\n",
    "    X_train - training set\n",
    "    Y_train - training label\n",
    "    X_test - test set\n",
    "    Y_test - test label\n",
    "    num_iteration - hyperparameter representing the number of iterations to optimize the parameters\n",
    "    learning_rate -- hyperparameter representing the learning rate used in the update rule of optimize()\n",
    "    print_cost -- Set to true to print the cost every 100 iterations\n",
    "    \n",
    "    Returns:\n",
    "    d - dictionary should contain following information w,b,training_accuracy, test_accuracy,cost\n",
    "    eg: d = {\"w\":w,\n",
    "             \"b\":b,\n",
    "             \"training_accuracy\": traing_accuracy,\n",
    "             \"test_accuracy\":test_accuracy,\n",
    "             \"cost\":cost}\n",
    "    \"\"\"\n",
    "    None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.选做题"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Congratulations on building your first logistic regression model. It is your time to analyze it further."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 4.1 Observe the effect of learning rate on the leraning process.   \n",
    "Hits: plot the learning curve with different learning rate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 4.2 Observe the effect of iteration_num on the test accuracy."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Challenge ! ! !\n",
    "\n",
    "The original data have images labeled 0,1,2,3,4,5,6,7,8,9. In our logistic model, we only detect if the digit in the image is larger or smaller than 5. Now, Let's go for a more challenging problem. Try to use softmax function to build a model to recognize which digit (0,1,2,3,4,5,6,7,8,9) is in the image."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Congratulations ! You have completed assigment 4. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
